{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sshar180/Machine-Learning-/blob/main/ANN_Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Overview \n",
        "In this assignment you will implement a two-layer neural network. You will implement the loss functions, gradients, optimizers to train the network and test its performance on MNIST dataset. \n",
        "\n",
        "For this assignment we will use the functionality of Pandas (https://pandas.pydata.org/), Matplotlib (https://matplotlib.org/), and Numpy (http://www.numpy.org/). \n",
        "\n",
        "If you are asked to **implement** a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, please ask. \n",
        "\n",
        "Before you start, make sure you have installed all those packages in your local Jupyter instance\n",
        "\n",
        "## Read *all* cells carefully and answer all parts (both text and missing code)\n",
        "\n",
        "You will complete all the code marked `TODO` and answer descriptive/derivation questions \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6urIY6Ci2D"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# make sure you import here everything else you may need"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vBsJizSAN5C"
      },
      "source": [
        "### Load MNIST Dataset \n",
        "\n",
        "For this assignment, we will use [MNIST](https://en.wikipedia.org/wiki/MNIST_database) handwritten digits data set. The dataset consists 10 handwritten digits (0,1,...,9). It is a widely used dataset to demonstrate simple image classification problem.\n",
        "\n",
        "MNIST dataset is publicly available from different sources. We will be using MNIST from Keras package. If you do not have Keras installed, you can find the installation guide [here](https://www.tutorialspoint.com/keras/keras_installation.htm). \n",
        "\n",
        "In short, you need to run ```conda install -c anaconda keras``` or ```pip install keras```\n",
        "\n",
        "The training data consists of 60000 images of size $28 \\times 28$ pixels; the test data consists of 10000 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdo3YbSzAN5D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "699b02aa-bbee-42bd-def4-a2e1b56b9330"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('Training data shape:',x_train.shape)\n",
        "print('Test data shape:',x_test.shape)\n",
        "\n",
        "n_img=10\n",
        "plt.figure(figsize=(n_img*2,2))\n",
        "plt.gray()\n",
        "for i in range(n_img):\n",
        "    plt.subplot(1,n_img,i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Training data shape: (60000, 28, 28)\n",
            "Test data shape: (10000, 28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x144 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " plt.imshow(x_train[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "TNOgbF1-6haE",
        "outputId": "dfb14d3b-f0ba-4659-d0e6-0c5dff62840f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f67076290d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv-Lawk-6if5",
        "outputId": "d676b7b9-9bfe-4397-97d3-9109acf9d1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw4N4norAN5D"
      },
      "source": [
        "We will be vectorizing the training and test images. So, the size of each vector will be 784."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOIMLKYtAN5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78d0de9-2bdb-4967-9406-c0607f31a461"
      },
      "source": [
        "x_train=x_train.reshape(x_train.shape[0],-1)\n",
        "x_test=x_test.reshape(x_test.shape[0],-1)\n",
        "\n",
        "print('Training data shape after reshaping:',x_train.shape)\n",
        "print('Test data shape after reshaping::',x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape after reshaping: (60000, 784)\n",
            "Test data shape after reshaping:: (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = x_train.reshape(x_train.shape[0],28,28)\n",
        "plt.imshow(p[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "_U6fZkla-auE",
        "outputId": "43cc9c14-617f-45c0-f2c2-05c417df2d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f670753b1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpnWo2lq-q-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHVQmrbIAN4-"
      },
      "source": [
        "## Question 1: Binary classification using neural network [45 pts]\n",
        "\n",
        "We will start with classification of images for two different digits using a two-layer network with a cross entropy loss. \n",
        "\n",
        "In the next question, we will extend the same architecture to multi-class classification. \n",
        "\n",
        "Pick any two digits out of ten for our classification (say 5 and 8), which we will assign label \"0\" or \"1\". \n",
        "\n",
        "Pick same number of images from each class for training and create arrays for input and output (say 1000). \n",
        "\n",
        "```\n",
        "# train_x -- N x 784 array of training input\n",
        "# train_y -- N x 1 array of binary labels \n",
        "```  \n",
        "\n",
        "If you use 1000 images from each class N = 2000. You can increase the number of training samples if you like. It is just a suggestion. \n",
        "\n",
        "\n",
        "We also need to transpose the dimension of the data so that their size becomes $784\\times N$. It will be helpful to feed it to our model based on our notations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fir2-wZWAN5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be78009-f95c-42d8-cea7-c6facd8917fc"
      },
      "source": [
        "def extract_binary_classification_dataset(x, y, label1, label2, num_samples):\n",
        "    \"\"\"Make a subset dataset from MNIST, containing only 2 classes for binary classification task \n",
        "    Args:\n",
        "        x (numpy.ndarray): data, can be x_train or x_test\n",
        "        y (numpy.ndarray): labels of data, can be y_train or y_test\n",
        "        label1 (int): the first class you pick, e.g. 5\n",
        "        label2 (int): the second class you pick, e.g. 8\n",
        "        num_samples (int): the number of images you select for each class, e.g. 1000\n",
        "    Returns:\n",
        "        x_ (numpy.ndarray): the data for 2 picked classes\n",
        "        y_ (numpy.ndarray): the corresponding labels for 2 picked classes\n",
        "    \"\"\"\n",
        "    # for class 1\n",
        "    x1 = x[y == label1]\n",
        "    x1 = x1[:num_samples]\n",
        "    y1 = np.zeros(len(x1))\n",
        "\n",
        "    # for class 2\n",
        "    x2 = x[y == label2]\n",
        "    x2 = x2[:num_samples]\n",
        "    y2 = np.ones(len(x2))\n",
        "\n",
        "    # combine 2 classes\n",
        "    x_ = np.concatenate((x1,x2),axis=0)\n",
        "    y_ = np.concatenate((y1,y2),axis=0)\n",
        "    return x_, y_\n",
        "\n",
        "\n",
        "# Pick your own digits\n",
        "label1 = 5\n",
        "label2 = 8\n",
        "num_samples = 1000\n",
        "\n",
        "# Train & test data\n",
        "train_x, train_y = extract_binary_classification_dataset(x_train, y_train, label1, label2, num_samples)\n",
        "test_x, test_y = extract_binary_classification_dataset(x_test, y_test, label1, label2, num_samples)\n",
        "\n",
        "# reshape data \n",
        "train_x = train_x.T\n",
        "test_x = test_x.T\n",
        "print(\"Training data shape:\", train_x.shape)\n",
        "print(\"Test data shape:\", test_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (784, 2000)\n",
            "Test data shape: (784, 1866)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vk = train_x.T"
      ],
      "metadata": {
        "id": "DVlQImjzAD0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWtFwd1KBCdH",
        "outputId": "91d8d35c-50a3-4dde-96a8-97d442610e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(figsize=(n_img*2,2))\n",
        "# plt.imshow(vk[7])"
      ],
      "metadata": {
        "id": "1vQdBbOV_2KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdeO0YieAN5E"
      },
      "source": [
        "### Network Architecture\n",
        "\n",
        "We will be using a two layer neural network in our experiment. The input layer will have 784 nodes, the hidden layer will have 256 nodes and the output layer will have 1 node. Each node will have $\\textit{sigmoid}$ activation function.\n",
        "\n",
        "The equations for feedforward operation will be the following:\n",
        "\n",
        "$$\\mathbf{z}^{(1)}=W^{(1)} \\mathbf{x}+ \\mathbf{b}^{(1)}\\\\\\mathbf{y}^{(1)}=\\varphi(\\mathbf{z}^{(1)})\\\\\\mathbf{z}^{(2)}=W^{(2)}  \\mathbf{y}^{(1)}+ \\mathbf{b}^{(2)} \\\\\\mathbf{y}^{(2)}=\\varphi(\\mathbf{z}^{(2)})$$\n",
        "\n",
        "where $\\mathbf{x}\\in \\mathbb{R}^{784}$ is the input layer, $\\mathbf{y}^{(1)}\\in \\mathbb{R}^{256}$ is the hidden layer, $\\mathbf{y}^{(2)} \\in \\mathbb{R}$ is the output layer, $W^{(1)}\\in \\mathbb{R}^{256\\times 784}$ is the first layer weights, $W^{(2)}\\in \\mathbb{R}^{1\\times 256}$ is the second layer weights, $\\mathbf{b}^{(1)}\\in \\mathbb{R}^{256}$ is the first layer bias, $\\mathbf{b}^{(2)}\\in \\mathbb{R}$ is the second layer bias, $\\varphi(\\cdot)$ is the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWw3zn2_AN5F"
      },
      "source": [
        "### Network initialization [5 pts]\n",
        "\n",
        "We initialize the weights for $W^{(1)}$ and $W^{(2)}$ with random values drawn from normal distribution with zero mean and 0.01 standard deviation. We will initialize bias vectors $\\mathbf{b}^{(1)}$ and $\\mathbf{b^{(2)}}$ with zero values. \n",
        "\n",
        "We can fix the seed for random initialization for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k52OUaoAN5F"
      },
      "source": [
        "def TwoLayerNetwork(layer_dims=[784,256,1]):\n",
        "    # Fix the seed\n",
        "    np.random.seed(3)\n",
        "\n",
        "    # TODO \n",
        "    # Your code goes here\n",
        "    input_layer =  layer_dims[0]\n",
        "    hidden_layer1= layer_dims[1]\n",
        "    output_layer = layer_dims[2]\n",
        "\n",
        "    params ={\n",
        "       'W1' : np.random.normal(0, 0.01, (hidden_layer1, input_layer)),\n",
        "       'b1' : np.zeros((hidden_layer1, 1)),\n",
        "       'W2' : np.random.normal(0, 0.01, (output_layer, hidden_layer1)),\n",
        "       'b2' : np.zeros((output_layer,1)),\n",
        "        }\n",
        "    \n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = TwoLayerNetwork()\n",
        "val['b1'].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHwqPCUMIxd8",
        "outputId": "2306fbc4-3e17-4e66-dd2f-1fa247eed564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xLDm_wkAN5F"
      },
      "source": [
        "### Sigmoid activation function \n",
        "Now we will write the sigmoid activation function as \n",
        "\n",
        "$$ \\varphi(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "Note that derivative of __sigmoid__ is $\\varphi'(z) = \\varphi(z) (1-\\varphi(z))$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CPxvM0UAN5F"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    # Input: Z -- numpy.ndarray\n",
        "    # TODO \n",
        "    # Write your function\n",
        "    Y = 1/(1+np.exp(-Z))\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGuGMhzeAN5G"
      },
      "source": [
        "### Cross entropy loss function [5 pts]\n",
        "We will minimize the binary cross entropy loss function. You will use the true labels and predicted labels of a batch of N samples. \n",
        "\n",
        "Binary crossentropy loss for $i^{th}$ sample can be written as \n",
        "\n",
        "$$Loss_i = -y_i \\log y^{(2)}_i- (1-y_i) \\log (1-y^{(2)}_i)$$\n",
        "\n",
        "where $y_i$ is the true label. We can find the average loss for a batch of N samples as $Loss=\\frac{1}{N}\\sum_{i=1}^{N} Loss_i$.\n",
        "\n",
        "Note that the gradient of the cross entropy loss w.r.t. the output is \n",
        "\n",
        "$$ \\nabla_{y^{(2)}} Loss_i = -\\frac{y_i}{y_i^{(2)}} + \\frac{1-y_i}{1-y_i^{(2)}} = \\frac{y_i^{(2)}-y_i}{y_i^{(2)}(1-y_i^{(2)})}.$$\n",
        "\n",
        "We can also show that $$\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} Loss_i  = \\nabla_{y^{(2)}} Loss_i \\odot \\varphi'(\\mathbf{z})= y_i^{(2)}-y_i,$$ \n",
        "where $\\odot$ denotes element-wise multiplication of the arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7KkfTXeAN5G"
      },
      "source": [
        "def CrossEntropyLoss(Y_true, Y2):\n",
        "    # TODO \n",
        "    # Write your code here\n",
        "    m = Y_true.shape[0]\n",
        "    loss = np.dot(Y_true,(np.log(Y2)).T) + np.dot((1-Y_true),np.log((1-Y2)).T)\n",
        "    loss = -loss/m\n",
        "    loss = float(np.squeeze(loss))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnFFFVP1AN5F"
      },
      "source": [
        "### Forward propagation  [5 pts]\n",
        "Next, we will write the code for the forward pass for two layer network. Each layer consists of an affine function (fully-connected layer) followed by an activation function. You wil also return the intermediate results ($\\mathbf{x}, \\mathbf{z}^{(1)}, \\mathbf{y}^{(1)}, \\mathbf{z}^{(2)}$) in addition to final output ($\\mathbf{y}^{(2)}$). You will need the intermediate outputs for the backpropagation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBHoC1_TAN5F"
      },
      "source": [
        "def forward(X, params):\n",
        "    \n",
        "    # TODO \n",
        "    # Write your codes here\n",
        "\n",
        "    # X -- 784 x N array \n",
        "    # params -- \n",
        "      # W1 -- 256 x 784 matrix\n",
        "      # b1 -- 256 x 1 vector\n",
        "      # W2 -- 1 x 256 matrix\n",
        "      # b2 -- 1 x 1 scalar \n",
        "    # Y2 -- 1 x N output \n",
        "    # intermediate -- X, Z1, Y1, Z2 \n",
        "      # Z1 -- 256 x N matrix\n",
        "      # Y1 -- 256 x N matrix\n",
        "      # Z2 -- 1 x N array\n",
        "\n",
        "      W1 = params['W1']\n",
        "      b1 = params['b1']\n",
        "      W2 = params['W2']\n",
        "      b2 = params['b2']\n",
        "\n",
        "      Z1 = np.dot(W1,X) + b1\n",
        "      Y1 = sigmoid(Z1)\n",
        "      Z2 = np.dot(W2,Y1) + b2\n",
        "      Y2 = sigmoid(Z2)\n",
        "\n",
        "      intermediate = {\n",
        "          'X':X,\n",
        "          'Z1':Z1,\n",
        "          'Y1':Y1,\n",
        "          'Z2':Z2\n",
        "      }\n",
        "      return Y2, intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ZM2NLAAN5G"
      },
      "source": [
        "### Backpropagration step [10 pts]\n",
        "Now we will implement the backpropagation step for the two layer neural network. \n",
        "\n",
        "You will need the gradient of the Loss w.r.t. $W^{(l)},\\mathbf{b}^{(l)}$ for $l = 1,2$ for all the training samples.  \n",
        "\n",
        "\n",
        "\n",
        "We saw that we can write the gradient of Loss with respect to $W^{(l)}, \\mathbf{b}^{(l)}$ for a single sample as\n",
        "\n",
        "$$\\nabla_{W^{(l)}} Loss_i = \\delta^{(l)} \\mathbf{y}^{(l-1)T},$$  \n",
        "$$\\nabla_{\\mathbf{b}^{(l)}} Loss_i = \\delta^{(l)},$$\n",
        "\n",
        "where \n",
        "$$\\delta^{(l)} = \\nabla_{\\mathbf{z}^{(l)}} Loss_i = \\nabla_{\\mathbf{y}^{(l)}} Loss_i \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n",
        "\n",
        "\n",
        "For the the last layer, we can compute $\\delta^{(L)}$ by plugging the value of $\\nabla_{\\mathbf{y}^{(L)}} Loss$ as described above. \n",
        "\n",
        "For the intermediate layers $l<L$, we can write \n",
        "$$\\delta^{(l)} = W^{(l+1)T}\\delta^{(l+1)} \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n",
        "\n",
        "\n",
        "\n",
        "**Once we have the gradients $\\nabla_{W^{(l)}} Loss_i, \\nabla_{\\mathbf{b}^{(l)}} Loss_i$ for all $i$. We can compute their average to compute the gradient of the total loss function $\\frac{1}{N} \\sum_{i=1}^N Loss_i$ as**\n",
        "\n",
        "$$\\nabla_{W^{(l)}} Loss = \\frac{1}{N} \\sum_i \\nabla_{W^{(l)}} Loss_i, $$\n",
        "$$ \\nabla_{\\mathbf{b}^{(l)}} Loss = \\frac{1}{N} \\sum_i  \\nabla_{\\mathbf{b}^{(l)}} Loss_i.$$\n",
        "\n",
        "**Please refer to the slides and lectures for more details.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODa4OXAqAN5G"
      },
      "source": [
        "def backward(Y_true, Y2, intermediate, params):\n",
        "    \n",
        "    # Inputs: \n",
        "      # Y_true -- 1 x N true labels\n",
        "      # Y2 -- 1 x N output of the last layer\n",
        "      # intermediate -- X, Z1, Y1, Z2 \n",
        "      # params -- W1, b1, W2, b2 \n",
        "    \n",
        "    # Outputs: \n",
        "      # grads -- [grad_W1, grad_b1, grad_W2, grad_b2]\n",
        "    \n",
        "    # TODO \n",
        "    # Write your codes here\n",
        "\n",
        "    W1 = params['W1']\n",
        "    W2 = params['W2']\n",
        "\n",
        "    Y1 = intermediate['Y1']\n",
        "    X = intermediate['X']\n",
        "    Z1 = intermediate['Z1']\n",
        "\n",
        "    # m = X.shape[1] # m = 2000\n",
        "    m = 1\n",
        "    dif = Y2 - Y_true\n",
        "    grad_W2 = (1/m) * np.dot(dif,Y1.T)\n",
        "    grad_b2 = (1/m) * np.sum(dif,axis=1, keepdims=True)\n",
        "\n",
        "    # dff2 = np.dot(W2.T, dif) * (1 - np.power(Y1, 2))\n",
        "    dff2 = np.dot(W2.T, dif) * (sigmoid(Z1) * (1 - sigmoid(Z1)))\n",
        "\n",
        "    grad_W1 = (1/m) * np.dot(dff2, X.T)\n",
        "    grad_b1 = (1/m) * np.sum(dff2, axis=1, keepdims=True)\n",
        "\n",
        "    grads = {\"grad_W1\": grad_W1,\n",
        "             \"grad_b1\": grad_b1,\n",
        "             \"grad_W2\": grad_W2,\n",
        "             \"grad_b2\": grad_b2}\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlHu8oIaAN5G"
      },
      "source": [
        "### Optimizer [5 pts]\n",
        "We will use a standard gradient descent-based optimizer to minimize the loss function. You have already implemented gradient descent in HW2. You may have to adjust learning rate that provides you best training/validation performance. In this exercise, we are not using validation data; in practice, you should use it to tune your hyperparameters such as learning rate, network architecture etc.\n",
        "\n",
        "You can use same learning rate for all weights in this assignment. \n",
        "\n",
        "You should update $W^1, \\mathbf{b}^1, W^2, \\mathbf{b}^2$ as \n",
        "$$ W^1 \\gets W^1 - \\alpha \\nabla_{W^1} Loss $$\n",
        "$$ \\mathbf{b}^1 \\gets \\mathbf{b}^1 - \\alpha \\nabla_{\\mathbf{b}^1} Loss $$ \n",
        "$$ W^2 \\gets W^2 - \\alpha \\nabla_{W^2} Loss $$ \n",
        "$$ \\mathbf{b}^2 \\gets \\mathbf{b}^2 - \\alpha \\nabla_{\\mathbf{b}^2} Loss $$ \n",
        "$\\alpha$ is the learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noqJ8U_PAN5G"
      },
      "source": [
        "def GD(params, grads, learning_rate):\n",
        "    \n",
        "    # updated params = old params - learning rate * gradient of Loss computed at old params\n",
        "    # TODO \n",
        "    # Write your codes here\n",
        "     # Retrieve each parameter from the dictionary \"params\"\n",
        "    W1 = params['W1']\n",
        "    b1 = params['b1']\n",
        "    W2 = params['W2']\n",
        "    b2 = params['b2']\n",
        "\n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    grad_W1 = grads['grad_W1']\n",
        "    grad_b1 = grads['grad_b1']\n",
        "    grad_W2 = grads['grad_W2']\n",
        "    grad_b2 = grads['grad_b2']\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate*grad_W1\n",
        "    b1 = b1 - learning_rate*grad_b1\n",
        "    W2 = W2 - learning_rate*grad_W2\n",
        "    b2 = b2 - learning_rate*grad_b2\n",
        "\n",
        "    params = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "                        \n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adhSifEGAN5G"
      },
      "source": [
        "### Train the Model [5 pts]\n",
        "We will train the model using the functions we wrote above. \n",
        "\n",
        "First, we specify the number of nodes in the layers, number of epochs and learning rate. Then we initialize the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKwAatEkAN5H"
      },
      "source": [
        "layer_dims = [train_x.shape[0],256,1]\n",
        "epochs = 100\n",
        "lr = 0.00001\n",
        "\n",
        "params = TwoLayerNetwork(layer_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdi_L3xYAN5H"
      },
      "source": [
        "Then we train the network for the number of epochs specified above. In every epoch, we will do the following:\n",
        "1. Calculate the forward pass to get estimated labels.\n",
        "2. Use the estimated labels calculate loss. We will be recording loss for every epoch.\n",
        "3. Use backpropagation to calculate gradients.\n",
        "4. Use gradient descent to update the weights and biases.\n",
        "\n",
        "You should store the loss value after every epoch in an array ```loss_history```  and print the loss value after every few epochs (say 20). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtEPxEleAN5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901c9654-560d-4232-cb24-5f00c67c587e"
      },
      "source": [
        "# TODO \n",
        "# Write your codes here\n",
        "Cost_per_Epoch = []\n",
        "for i in range(0, epochs):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"train_x, params\" and Outputs: \"Y2, intermediate\".\n",
        "        Y2, intermediate = forward(train_x, params)\n",
        "        \n",
        "        # Cost function. Inputs: \"train_y, Y2, parameters\". Outputs: \"cost\".\n",
        "        cost = CrossEntropyLoss(train_y, Y2[0])\n",
        "        Cost_per_Epoch.append(cost)\n",
        " \n",
        "        # Backpropagation. Inputs: \"train_y, Y2, intermediate, params\". Outputs: \"grads\".\n",
        "        grads = backward(train_y, Y2, intermediate, params)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"params, grads,l r\". Outputs: \"params\".\n",
        "        params = GD(params, grads, lr)\n",
        "\n",
        "        # Print the cost for every iterations\n",
        "        print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.691340\n",
            "Cost after iteration 1: 0.672342\n",
            "Cost after iteration 2: 0.656256\n",
            "Cost after iteration 3: 0.640605\n",
            "Cost after iteration 4: 0.624871\n",
            "Cost after iteration 5: 0.609149\n",
            "Cost after iteration 6: 0.593113\n",
            "Cost after iteration 7: 0.576539\n",
            "Cost after iteration 8: 0.559499\n",
            "Cost after iteration 9: 0.542371\n",
            "Cost after iteration 10: 0.524982\n",
            "Cost after iteration 11: 0.507521\n",
            "Cost after iteration 12: 0.490128\n",
            "Cost after iteration 13: 0.472892\n",
            "Cost after iteration 14: 0.455958\n",
            "Cost after iteration 15: 0.439394\n",
            "Cost after iteration 16: 0.423240\n",
            "Cost after iteration 17: 0.407555\n",
            "Cost after iteration 18: 0.392382\n",
            "Cost after iteration 19: 0.377890\n",
            "Cost after iteration 20: 0.364140\n",
            "Cost after iteration 21: 0.350387\n",
            "Cost after iteration 22: 0.337798\n",
            "Cost after iteration 23: 0.325879\n",
            "Cost after iteration 24: 0.314708\n",
            "Cost after iteration 25: 0.303992\n",
            "Cost after iteration 26: 0.294039\n",
            "Cost after iteration 27: 0.284477\n",
            "Cost after iteration 28: 0.275476\n",
            "Cost after iteration 29: 0.267052\n",
            "Cost after iteration 30: 0.258965\n",
            "Cost after iteration 31: 0.251426\n",
            "Cost after iteration 32: 0.244272\n",
            "Cost after iteration 33: 0.237515\n",
            "Cost after iteration 34: 0.231097\n",
            "Cost after iteration 35: 0.225050\n",
            "Cost after iteration 36: 0.219316\n",
            "Cost after iteration 37: 0.213976\n",
            "Cost after iteration 38: 0.208768\n",
            "Cost after iteration 39: 0.203908\n",
            "Cost after iteration 40: 0.199072\n",
            "Cost after iteration 41: 0.194578\n",
            "Cost after iteration 42: 0.190143\n",
            "Cost after iteration 43: 0.186009\n",
            "Cost after iteration 44: 0.181939\n",
            "Cost after iteration 45: 0.178185\n",
            "Cost after iteration 46: 0.174539\n",
            "Cost after iteration 47: 0.171024\n",
            "Cost after iteration 48: 0.167658\n",
            "Cost after iteration 49: 0.164463\n",
            "Cost after iteration 50: 0.161365\n",
            "Cost after iteration 51: 0.158388\n",
            "Cost after iteration 52: 0.155451\n",
            "Cost after iteration 53: 0.152767\n",
            "Cost after iteration 54: 0.150055\n",
            "Cost after iteration 55: 0.147560\n",
            "Cost after iteration 56: 0.144986\n",
            "Cost after iteration 57: 0.142779\n",
            "Cost after iteration 58: 0.140430\n",
            "Cost after iteration 59: 0.138396\n",
            "Cost after iteration 60: 0.136193\n",
            "Cost after iteration 61: 0.134362\n",
            "Cost after iteration 62: 0.132305\n",
            "Cost after iteration 63: 0.130534\n",
            "Cost after iteration 64: 0.128522\n",
            "Cost after iteration 65: 0.126851\n",
            "Cost after iteration 66: 0.124917\n",
            "Cost after iteration 67: 0.123207\n",
            "Cost after iteration 68: 0.121323\n",
            "Cost after iteration 69: 0.119704\n",
            "Cost after iteration 70: 0.118014\n",
            "Cost after iteration 71: 0.116448\n",
            "Cost after iteration 72: 0.114900\n",
            "Cost after iteration 73: 0.113553\n",
            "Cost after iteration 74: 0.112118\n",
            "Cost after iteration 75: 0.110807\n",
            "Cost after iteration 76: 0.109399\n",
            "Cost after iteration 77: 0.108233\n",
            "Cost after iteration 78: 0.106883\n",
            "Cost after iteration 79: 0.105759\n",
            "Cost after iteration 80: 0.104429\n",
            "Cost after iteration 81: 0.103397\n",
            "Cost after iteration 82: 0.102070\n",
            "Cost after iteration 83: 0.101103\n",
            "Cost after iteration 84: 0.099778\n",
            "Cost after iteration 85: 0.098859\n",
            "Cost after iteration 86: 0.097574\n",
            "Cost after iteration 87: 0.096707\n",
            "Cost after iteration 88: 0.095463\n",
            "Cost after iteration 89: 0.094636\n",
            "Cost after iteration 90: 0.093443\n",
            "Cost after iteration 91: 0.092631\n",
            "Cost after iteration 92: 0.091534\n",
            "Cost after iteration 93: 0.090752\n",
            "Cost after iteration 94: 0.089751\n",
            "Cost after iteration 95: 0.089022\n",
            "Cost after iteration 96: 0.088083\n",
            "Cost after iteration 97: 0.087408\n",
            "Cost after iteration 98: 0.086537\n",
            "Cost after iteration 99: 0.085956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-7VVZ6iAN5H"
      },
      "source": [
        "Now we will plot the recorded loss values vs epochs. We will observe the training loss decreasing with the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu8zTsABAN5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "d594b624-f374-4b96-b0cf-f1cde72b36b9"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(Cost_per_Epoch)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZlLv1NbAN5H"
      },
      "source": [
        "### Evaluation on test data [5 pts]\n",
        "\n",
        "Now we will be evaluating the accuracy we get from the trained model. We feed training data and test data to the forward model along with the trained parameters. \n",
        "\n",
        "Note that, we need to covert the output probability of the forward pass to binary labels before evaluating accuracy. Since the model provides the posterior probability $p(y = 1 | x)$ in range [0,1]. We can binarize them using 0.5 as a theshold (i.e. if $y_i^{(2)}\\geq 0.5$, $y_i^{(2)} \\gets 1$ otherwise  $y_i^{(2)} \\gets 0$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKZ7BSZjAN5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f5269b-c511-4df4-b8fa-bc89b7da4e7e"
      },
      "source": [
        "# TODO \n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    A2, cache = forward(X, parameters)\n",
        "    threshold = 0.5 \n",
        "    p = (A2 > threshold)\n",
        "    \n",
        "    return p\n",
        "\n",
        "predictions_test = predict(params, test_x)\n",
        "predictions_train = predict(params, train_x)\n",
        "\n",
        "print(\"Training accuracy:\",float((np.dot(train_y,predictions_train.T) + np.dot(1-train_y,1-predictions_train.T))/float(train_y.size)*100))\n",
        "print(\"Test accuracy:\",float((np.dot(test_y,predictions_test.T) + np.dot(1-test_y,1-predictions_test.T))/float(test_y.size)*100))\n",
        "\n",
        "# cross-check Using Sklearn \n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Training accuracy:\",accuracy_score(train_y,predictions_train[0])*100)\n",
        "print(\"Test accuracy:\",accuracy_score(test_y,predictions_test[0])*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 98.25\n",
            "Test accuracy: 95.12325830653805\n",
            "Training accuracy: 98.25\n",
            "Test accuracy: 95.12325830653805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba-laCd-4OkR"
      },
      "source": [
        "### Visualize some of the correct/miscalassified images [5 pts]\n",
        "\n",
        "Now we will look at some images from training and test sets that were misclassified. \n",
        "\n",
        "Training set. \n",
        "Pick 5 images from each class that are correcly and incorreclty classified. \n",
        "True/False Positive/Negatives\n",
        "\n",
        "Test set. \n",
        "Pick 5 images from each class that are correcly and incorreclty classified. \n",
        "True/False Positive/Negatives\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O5zZTCn4N18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "fb1963f5-b619-457e-e1a1-78b580b1f43f"
      },
      "source": [
        "# TODO \n",
        "# Training set\n",
        "print(\"Training set examples for true/false positive/negative\")\n",
        "Y_hat, caches = forward(train_x, params)\n",
        "\n",
        "# your code goes here...\n",
        "predict = (Y_hat > .5)*1.0\n",
        "a=predict[0]\n",
        "b=train_y\n",
        "false_prediction = np.where(np.logical_and(a ==0 ,b==1))[0]\n",
        "\n",
        "train_x.shape\n",
        "p = train_x.T\n",
        "p = p.reshape(p.shape[0],28,28)\n",
        "print(\"Training set negative examples\")\n",
        "for x in range(0,5):\n",
        "  index = false_prediction[x]\n",
        "  # plt.imshow(p[index])\n",
        "  print(\"Predicted Value\", a[index],\"Predicted Label: 5\",\"True Value\",b[index],\" True Label: 8\", \"Index\", index)\n",
        "\n",
        "n_img=5\n",
        "plt.figure(figsize=(n_img*2,2))\n",
        "plt.gray()\n",
        "for i in range(n_img):\n",
        "  index = false_prediction[i]\n",
        "  plt.subplot(1,n_img,i+1)\n",
        "  plt.imshow(p[i])\n",
        "plt.show()  \n",
        "\n",
        "# Positive Predictions\n",
        "print(\"Training set positive examples\")\n",
        "true_prediction = np.where(np.logical_and(a ==1 ,b==1))[0]\n",
        "for x in range(0,5):\n",
        "  index = true_prediction[x]\n",
        "  # plt.imshow(p[index])\n",
        "  print(\"Predicted Value\", a[index],\"Predicted Label: 5\",\"True Value\",b[index],\" True Label: 5\", \"Index\", index)\n",
        "\n",
        "n_img=5\n",
        "plt.figure(figsize=(n_img*2,2))\n",
        "plt.gray()\n",
        "for i in range(n_img):\n",
        "  index = true_prediction[i]\n",
        "  plt.subplot(1,n_img,i+1)\n",
        "  plt.imshow(p[i])\n",
        "plt.show()  \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set examples for true/false positive/negative\n",
            "Training set negative examples\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 1041\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 1125\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 1237\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 1242\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 1247\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set positive examples\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 1000\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 1001\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 1002\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 1003\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 1004\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=predict[0]\n",
        "b=train_y\n",
        "t = np.where(np.logical_and(a ==0 ,b==1))[0]"
      ],
      "metadata": {
        "id": "BYs_UVPL0L1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(train_y,predict[0])*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfe7XiFlofM3",
        "outputId": "dc65bc5e-912c-4871-8136-b7655e689086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.25"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpj1h_wh_y6r",
        "outputId": "3fa1e9ae-9e66-43f9-82c2-740dcda8652d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape\n",
        "p = train_x.T"
      ],
      "metadata": {
        "id": "M-zYy3yLBjEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[1041]\n",
        "predict[0][1041]\n",
        "p = p.reshape(p.shape[0],28,28)\n",
        "plt.imshow(p[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "YxDtvqtB9JNg",
        "outputId": "a4cf311f-f5c1-4058-f37c-e7a8d856bb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6704ee02e0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh1OZ4CUC5lX",
        "outputId": "06e7f8ce-0642-4743-dcf7-02e2c76e68ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYr5U4Id5LU5",
        "outputId": "1f769988-c1be-4404-d3d3-6f24cad41df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1041, 1125, 1237, 1242, 1247, 1250, 1251, 1256, 1293, 1436, 1477,\n",
              "       1478, 1517, 1556, 1607, 1630, 1668, 1730, 1840, 1870, 1914, 1916,\n",
              "       1939])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jym9VlP-_0rd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "a903ed14-9944-49b0-a89d-b5c0f4efebe2"
      },
      "source": [
        "# Test set\n",
        "print(\"Test set examples for true/false positive/negative\")\n",
        "Y_hat, caches = forward(test_x, params)\n",
        "\n",
        "# your code goes here...\n",
        "predict = (Y_hat > .5)*1.0\n",
        "a=predict[0]\n",
        "b=test_y\n",
        "false_prediction = np.where(np.logical_and(a ==0 ,b==1))[0]\n",
        "\n",
        "# train_x.shape\n",
        "p = test_x.T\n",
        "p = p.reshape(p.shape[0],28,28)\n",
        "print(\"Training set negative examples\")\n",
        "for x in range(0,5):\n",
        "  index = false_prediction[x]\n",
        "  # plt.imshow(p[index])\n",
        "  print(\"Predicted Value\", a[index],\"Predicted Label: 5\",\"True Value\",b[index],\" True Label: 8\", \"Index\", index)\n",
        "\n",
        "n_img=5\n",
        "plt.figure(figsize=(n_img*2,2))\n",
        "plt.gray()\n",
        "for i in range(n_img):\n",
        "  index = false_prediction[i]\n",
        "  plt.subplot(1,n_img,i+1)\n",
        "  plt.imshow(p[i])\n",
        "plt.show()  \n",
        "\n",
        "# Positive Predictions\n",
        "print(\"Training set positive examples\")\n",
        "true_prediction = np.where(np.logical_and(a ==1 ,b==1))[0]\n",
        "for x in range(0,5):\n",
        "  index = true_prediction[x]\n",
        "  # plt.imshow(p[index])\n",
        "  print(\"Predicted Value\", a[index],\"Predicted Label: 5\",\"True Value\",b[index],\" True Label: 5\", \"Index\", index)\n",
        "\n",
        "n_img=5\n",
        "plt.figure(figsize=(n_img*2,2))\n",
        "plt.gray()\n",
        "for i in range(n_img):\n",
        "  index = true_prediction[i]\n",
        "  plt.subplot(1,n_img,i+1)\n",
        "  plt.imshow(p[i])\n",
        "plt.show()  \n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set examples for true/false positive/negative\n",
            "Training set negative examples\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 936\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 953\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 959\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 962\n",
            "Predicted Value 0.0 Predicted Label: 5 True Value 1.0  True Label: 8 Index 965\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAACBCAYAAAAPH4TmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAW8UlEQVR4nO3df7BVVd3H8c8CxRB1AlQyfmimYDaVChpPjxSNmIkUmFHUZDyVkCIzqNTED0ubCH9G5fiMDamIRBnJD2l0RCQLLUAQyAcuKGiimIKokyQKovv5g9NyrSXncva9++yz97rv14xzv/t+z717zf3edVmetfZaJkkSAQAAoHbtGt0AAACAsmEABQAAkBIDKAAAgJQYQAEAAKTEAAoAACAlBlAAAAAptWoAZYz5vDHmCWPMZmPMhKwahcagnvGglnGhnvGglvEwLd0HyhjTXtKTks6WtFXSSklfS5KkKbvmIS/UMx7UMi7UMx7UMi4HteJrz5C0OUmSpyXJGHOXpKGSqv4iGGPYtbPBkiQxVVKp6kktGy+rWlZeQz0bjL4ZD/pmXKrVszVTeN0lPedcb618zmOMGW2MWWWMWdWKe6H+DlhPalka9M240DfjQd+MSGvegapJkiTTJU2XGEmXHbWMC/WMB7WMC/Ush9a8A/W8pJ7OdY/K51BO1DMe1DIu1DMe1DIirRlArZR0ojHmQ8aYDpJGSFqYTbPQANQzHtQyLtQzHtQyIi2ewkuSZK8xZqykRZLaS7o9SZL1mbUMuaKe8aCWcaGe8aCWcWnxNgYtuhlzuQ3XzNMhqVDLxsuqlhL1LAL6Zjzom3Gpx1N4AAAAbRIDKAAAgJQYQAEAAKTEAAoAACAlBlAAAAApMYACAABIqe5HubRVnTt3tnGvXr1q/rotW7bY+PLLL/dy69ats/GTTz7p5f7+97+nbWKb0LdvXxsPGzbMxhdccIH3uj59+tjYGP+JVXerj9WrV3u5DRs22Hjq1KlebuPGjekbDAAoBd6BAgAASIkBFAAAQEpM4bXCeeedZ+MvfvGLXm7gwIE2PuGEE2r+nu7U3LHHHuvlDjnkkKpf1759+5rvUUajR4+28UknneTlBgwYUPXrTjvtNBu7U3HNTdNNnz7dy82fP9/GDzzwQI0tRlGcffbZ3rU7fTt8+HAv5069h9zfmfAEh3feecfG/fv393KrVq2qvbFtyKxZs2zsLnNwp8Ul6ZFHHqmaq9WOHTu8a3epBOLWsWNHGw8aNMjLPf/8u+c4h8szasE7UAAAACkxgAIAAEiJARQAAEBKJpzLr+vNSnKq9Ic//GEbX3rppTYeNWqU9zp3bjVcU5O3WtdAlfXEd3eNSfg7u2vXLhuHWwc8/PDD+8299NJL3uvcdU5l0dZPfP/qV79q4yFDhni5wYMH2/j973+/l3P76ubNm73cggULbLxixQov524jMn78eC930UUX2fi6667zchMnTtxf89+jrH2zVkcddZR3/eijj9rYXQMV9u/m1p7VmnP/DkjSL3/5SxvXo++39b6ZhXbt/Pd33LWu4TrDfv362ficc87xcu7vQbiG7pZbbrHxjBkzqralWj15BwoAACAlBlAAAAApsY3BfvTo0cPG48aNq+u9wimn9evX1/V+ZTVv3jwbuzuKS/7P8PTTT8+rScjB9ddf712PHTvWxu62HuEUursdSLj1xM9//nMbr1mzxsu99dZbNbXLnX6SpBNPPNHGV155ZU3fo60JT2Rwry+55BIbh9uIuFN/559/vpdz+364vYmrqanJu548ebKNw2kdThBovVNOOcXG7lYBkr984gMf+ICXGzlypI3DLUbcLWnC/r5161YbT5s2rer9br31Vi+3e/fu/ba/VrwDBQAAkBIDKAAAgJQYQAEAAKQU9TYGRx55pI3DtUx//etfbXz//fd7OfcRyfvuu8/Gr7/+uve6Tp062ThcZ+E+8hw+Du2uu3jjjTe8XHiPrJX1UWl3HUS4/sStg/s4qyQ9++yz9W1YA7WFR6W3bdvmXbu/B3/4wx9sfMMNN3ive/zxx228Z8+ezNvlbnUi+es83nzzzRZ9z7L2zVr17dvXu3b/Lo4ZM8bG4RqoejjzzDNt7B4Vk5W20DfDNW233Xabjc866ywbb9++3Xvdq6++auOuXbt6Offf7J07d3o5dx3snDlzvJz7u/TKK68csO1psY0BAABARhhAAQAApBTVNgbuVI7kT6t94hOf8HLh47Cu5cuX29h9dPKZZ57xXue+hek+Rin5O2ej9dxHUcO3+KdMmWJj9y1gKe4pvLbAnWqX/C0s7r33XhuvWrUqryZJkp566qlc7xejRp7eUI9pu7agS5cuNl68eLGXc7fycB199NHetTttF/59vuqqq2y8aNEiL/f000+na2wOeAcKAAAgJQZQAAAAKR1wAGWMud0Ys90Ys875XBdjzGJjzKbKx871bSayQj3jQS3jQj3jQS3bhlrWQN0h6WZJdzqfmyBpSZIk1xpjJlSuf5B98w6sQ4cONv7tb3/r5dx1T1OnTvVyDz74YE3fP1z35Crp+po7VOB61iI8pdtdS/GRj3ykaq457nEOu3btakXrcnWHSl7L/endu7eNw5PV//GPf9h4/vz5ubUpJ3cownr+R9g389xCpwHuUAS1dNc8Sf66w2prniT/SKTf//73Xm7GjBk2fuihh1rbxIY64DtQSZIslRRurDBU0sxKPFPSsGybhXqhnvGglnGhnvGglm1DS5/C65YkyQuV+EVJ3aq90BgzWtLoFt4H+aipntSyFOibcaFvxoO+GZlWb2OQJEnS3E6pSZJMlzRdymZH1cMOO8y7njhxoo2HDBni5Xbs2GHjG2+80cuVaJomV83VM+tapuHuQH3RRReF7bLxzJkzvZw7hee+Lpzac6eDZs+eXTVXJnn3zaxcfPHFNu7YsaOXc08NCHcqjl1R+2atwq1jGrmNQaOVpW+G02+f/OQnq77W3XLkG9/4ho23bNmSfcMKoqVP4W0zxhwjSZWP2w/wehQb9YwHtYwL9YwHtYxMSwdQCyWNrMQjJd2TTXPQINQzHtQyLtQzHtQyMrVsY/A7Scsk9THGbDXGfEfStZLONsZskjSoco0SoJ7xoJZxoZ7xoJZtg8nzUdIs5nLduVXJX/MSbiswYMAAG4dHrbRVZTnx3V3zJEl/+ctfbNynTx8vt3r1ahu72xFI1Y9sGDVqlHftHgFz7LHHejm3j5xxxhlerpHbH8R64vstt9xi4+9+97te7oorrrDxL37xi7yalIuy9M2Wevvtt71rt18999xzNnbXrh6Iuz1NkdYqlrlv9u/f38bumkNJOuKII2wcrhUdOXKkjWM7yqxaPdmJHAAAICUGUAAAACm1ehuDvH3qU5+qmluzZo13zbRdeYXTdO71vHnzvNzw4cNTf//p06d71+4UXjhNPGzYMBs/+uijXq6pqalqOzZu3Ji6XZDOO+88G//73//2cgsWLMi5NWiNyZMn27i5bQtq3dLA7aeSNHfuXBufe+65Xm7RokU1fU/43BM93Ck7SZo1a5aNw+1kYpu2qwXvQAEAAKTEAAoAACCl0j2Ft327v/dY165dbbx7924vd91119n4nnv8LTfWrl3b2qaUUuxP+tTb6NH+6Qru03zh03vulMJjjz2WeVvK/KSPKzxdwJ1637Rpk5f71re+lfr7b9682bt+8803U3+PPMTQN0866STveuXKlTY+9NBDvdxPf/pTG9900002bu4pvHAKb9u2bTZ+6aWXvNzAgQNtnPd0epn65vHHH+9du/82dujQwct9+ctftrG7fEHy/9517959v18j+U8u33333V5uyZIlNv7nP/95oKbnhqfwAAAAMsIACgAAICUGUAAAACmVbg1U2N5aH50MX/erX/3KxsuXL/dyvXr1srG7fmL9+vVVv/9HP/pR73rZsmU2LtJ2CjGssygSd02Gu1u65K/Pu+SSS7xcFrsml2mdRXM+97nPedfh7setFW5vcu21756g8cc//tHLNXJ9VAx90/27KvlrBB944AEvF2470BI/+clPbDxp0iQv5/a5cNuSeitT3/z+97/vXbtrh/fs2ePl3HXG7dr577906tTJxi+//LKNw61IOnfubONwm4RXXnnFxmE9866hizVQAAAAGWEABQAAkFLppvBuuOEG79o9XLRI3Edq//znP3u5ESNG5Nyad8UwTVBUn/70p73rn/3sZzYOtzhwD0Ft6aG4ZZomaM4JJ5zgXd988802/te//uXlwi0J/qNnz57e9amnnmrjcHrdddddd3nX3/72t22c93ReDH2zb9++3rW7W/UPf/hDL5fm0OBa7rdixQovN2bMGBszhVfd3/72N+/aPUw45G5L4S5TkaQ5c+bY2O2n4dZD7t/CcePGebmxY8fa+NVXX/VyV111lY3DqeJ6YwoPAAAgIwygAAAAUmIABQAAkFLp1kC1b9/eu3bXOrinSEvSQQcdZONwjUT4CGY9hT/jq6++2sZTpkzJrR2VtpR+nUVZNLfFQZ8+fWzs/p6mUaZ1Fnnr2LGjjU8++WQv96Mf/cjGX/jCF7ycu87CfUQ+DzH0zaOOOsq7Do9Xydrll19u4xtvvNHLsY1Bbbp06eJdu0fs/OlPf/Jy7lYU4frELLh9bvLkyV7OPY7tS1/6kper9ziGNVAAAAAZYQAFAACQUsvmDhro7bff9q5XrVpl4969e1f9urPOOsu7Pvjgg23sTqlJ0umnn96KFr6XMf67f+GjvoiT+5j2I4884uXCU+uRrTfeeMPGL774opc77rjjqn5dvaecYnT++efbePTo0V4ui93GmzNs2DAb57kcJSbu7t/Se09NyNO2bduq5oYOHWrj973vfV7O7e954h0oAACAlBhAAQAApMQACgAAIKXSrYFqqSVLllTNnXLKKd61uwZq7969Np4xY4b3ul//+tc2vuyyy7zc17/+9Ra0EjFx1zm5azUkqampKefWtC2f+cxnbHzTTTd5uY997GM2DtemzZ49u74Ni9CkSZNsnMXxLM0Jt0kYMGCAjcM1UEuXLq1rW9B64XrEUaNGVX3twoULbZz3MUvV8A4UAABASgccQBljehpjHjLGNBlj1htjxlU+38UYs9gYs6nysXP9m4vWopbxoG/GhVrGg77ZNtQyhbdX0vgkSVYbYw6X9JgxZrGk/5G0JEmSa40xEyRNkPSD+jW1ftzdVSV/J1Z3l+jw7UX3FPmBAwfWfL+tW7embGGmSllLd8fh8FHz3/zmN3k3Z7/cU8Yl//fo0EMP9XLDhw/P4pbR980OHTp413v27LGx+zN1dxCXpDFjxti4U6dOXm758uU2vuKKK7zczp07W97Y1itFLcNpNHfHfXdZQz3ud99993k5d9rO7W+StHHjxszbkkL0fbOl3H8rZ82a5eW6d+9u4/Dv/JVXXmnjomxZccB3oJIkeSFJktWVeKekDZK6SxoqaWblZTMlDatTG5EhahkP+mZcqGU86JttQ6pF5MaY4ySdKmmFpG5JkrxQSb0oqVuVrxktafT+cmgcahkX6hkPahkX6hmvmheRG2MOkzRX0mVJkrzm5pJ976ft9z21JEmmJ0nSL0mSfq1qKTJDLeNCPeNBLeNCPeNW0ztQxpiDte+XYHaSJPMqn95mjDkmSZIXjDHHSNper0bW24YNG7zrOXPm2PgrX/lK1a/77Gc/WzXnHjlz7733erkJEyakbWJmylJL93gIyT9pPTxZPes1UOEaj7At1XKnnXaal9u+/d0f4ze/+U0vl9X6jLLUszmHH364dz148GAbH3/88V7ugx/8oI2HDBli43D92e7du2384x//2MtNmzbNxg1e8+QpSy3DtSkvv/yyjcO+0xLhMUfu2qawj61evdrG4XYVjVaWembliCOOsPEFF1xg43C956BBg2zsrjGW/KNcLr74Yi+3bt26TNqZpVqewjOSbpO0IUmSaU5qoaSRlXikpHuybx7qgFpGgr4ZHWoZCfpm21DLO1D/LelCSf9njFlb+dwkSddKmmOM+Y6kLZKqv1WDIqGW8aBvxoVaxoO+2QaYPB8HNMYU49nDA+jW7d11fbfeequN+/Xzp6OPPvpoGz/zzDNezn088+qrr862ga2QJInJ4vvUu5bhtNm8efNs/M4773g5dwrBfZ0k7fsfwX3cqYFwx2R3p3D3ayT/kdkw507/Llq0yMtNnTq16v2ykFUtpfrXs3379t71+PHjbTxlyhQv505v9u7d28u52xq4vwcrV670XueeKL927dr0DW6AsvTN0J133mljd2dwSbrmmmtsPH/+fC/n9nH368Jd+93tKsL+7da53rugp1H0vtmjRw8bNzcGcKfJJX+aLpxeP+ecc2wc7jDueu21d5eC3X777V7OnV5v8HY/nmr1ZCdyAACAlBhAAQAApMQACgAAICXWQKVw4YUXetf9+/e3cfiotPsIe5GUdZ2FO78erpFwhWun3Meqm5qabOyum5L8tUzhWopw7YbLXa+za9euqq+rh6Kvs3Bdf/313vX3vve9mr5u79693vWaNWts7B7fcv/997eidcVQ1r7pri1cunSpl+vatauN27Xz/3/dXcPm5ubOneu9bvbs2TZuri8WSdH7prsO6ZBDDgnvZ+Nw7aJ7lFK4FtXtq8uWLbPx3Xff7b3OPTrt2WefTdPshmENFAAAQEYYQAEAAKTEFF4bU9ZpArxX0acJXEOHDvWuR4wYYeNevXp5OXdH6QULFng5d4fx2MTQN8Najho1ysbhFgfudNzDDz9s43CX/rynxrNQpr758Y9/3Lt2p1M7d+7s5bZs2WJjdxpQKtY2ElljCg8AACAjDKAAAABSYgAFAACQEmug2pgY1llgnzKts8CB0TfjQd+MC2ugAAAAMsIACgAAICUGUAAAACkxgAIAAEiJARQAAEBKDKAAAABSYgAFAACQEgMoAACAlBhAAQAApHRQzvfbIWmLpCMrcaO1tXYcm+H3opbV5dGWLGsp7Wvv62pbP8Na0DdbryjtkOibWShKPRveN3M9ysXe1JhVSZL0y/3GtCNzRWl7UdohFastaRSp3UVpS1Ha0RJFaXtR2iEVqy1pFKndRWlLEdrBFB4AAEBKDKAAAABSatQAanqD7huiHa1XlLYXpR1SsdqSRpHaXZS2FKUdLVGUthelHVKx2pJGkdpdlLY0vB0NWQMFAABQZkzhAQAApMQACgAAIKVcB1DGmM8bY54wxmw2xkzI+d63G2O2G2PWOZ/rYoxZbIzZVPnYOYd29DTGPGSMaTLGrDfGjGtUW1qDWsZTS4l6Vu4ZRT2pZTy1lKhnkWuZ2wDKGNNe0v9KOlfSyZK+Zow5Oa/7S7pD0ueDz02QtCRJkhMlLalc19teSeOTJDlZUn9Jl1Z+Do1oS4tQS6v0tZSop6P09aSWVulrKVHPiuLWMkmSXP6T9F+SFjnXEyVNzOv+lXseJ2mdc/2EpGMq8TGSnsizPZX73iPp7CK0hVq2vVpSz7jqSS3jqSX1LH4t85zC6y7pOed6a+VzjdQtSZIXKvGLkrrleXNjzHGSTpW0otFtSYlaBkpcS4l6vkeJ60ktAyWupUQ9PUWrJYvIK5J9w9jc9nQwxhwmaa6ky5Ikea2RbYkNtYwL9YwHtYxLnj/DItYyzwHU85J6Otc9Kp9rpG3GmGMkqfJxex43NcYcrH2/CLOTJJnXyLa0ELWsiKCWEvW0IqgntayIoJYS9VTlPoWsZZ4DqJWSTjTGfMgY00HSCEkLc7z//iyUNLISj9S+udW6MsYYSbdJ2pAkybRGtqUVqKWiqaVEPSVFU09qqWhqKVHPYtcy58VfgyU9KekpSZNzvvfvJL0g6S3tm0f+jqSu2rd6f5OkByV1yaEdZ2rfW42PS1pb+W9wI9pCLakl9YyvntQynlpSz2LXkqNcAAAAUmIROQAAQEoMoAAAAFJiAAUAAJASAygAAICUGEABAACkxAAKAAAgJQZQAAAAKf0/Po3aDKgFAnAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set positive examples\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 892\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 893\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 894\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 895\n",
            "Predicted Value 1.0 Predicted Label: 5 True Value 1.0  True Label: 5 Index 896\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBEj5ST72SG6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoli2hpzp2gy"
      },
      "source": [
        "## Question 2. Multiclass classification [35 pts] \n",
        "\n",
        "Now we will build a classifier to separate all the digits. For this purpose, we will only change the last layer and the loss. \n",
        "\n",
        "\n",
        "Instead of using a single output, we will provide 10 outputs; and instead of using a binary cross entropy loss, we will use mutli-class cross entropy loss. \n",
        "\n",
        "In multinomal logistic regression (aka softmax regression), we define the posterior probability of label $y \\in \\{0,\\ldots, K-1\\}$ as \n",
        "\n",
        "\n",
        "$$p(y = c | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_c^T\\mathbf{x})}{\\sum_{k=1}^K \\exp(\\mathbf{w}_k^T\\mathbf{x})} = \\mathbf{p}_c.$$ \n",
        "\n",
        "In other words, last layer of the network provides a probability vector $\\mathbf{p} \\in \\mathbb{R}^K$, such that each $0 \\le \\mathbf{p}_c \\le 1$ and $\\sum_c \\mathbf{p}_c = 1$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxGPKVAUBO-w"
      },
      "source": [
        "### Softmax function [5 pts]\n",
        "\n",
        "Let us first define the softmax function, which is a multinomal extension of the sigmoid function that maps a vector of length $K$ to a probability vector. \n",
        "\n",
        "We can define ```softmax``` function on a vector $\\mathbf{z} \\in \\mathbb{R}^K$ as $\\mathbf{p} = \\text{softmax}(\\mathbf{z})$: \n",
        "\n",
        "$$\\mathbf{p}_c(\\mathbf{z}) = \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wezeelLcBSCo"
      },
      "source": [
        "def softmax(Z):\n",
        "    # Z -- K x N numpy.ndarray, K is the number of classes, N is the number of samples\n",
        "    # TODO  \n",
        "    # your code goes here... \n",
        "\n",
        "      exp =   np.exp(Z)\n",
        "      probs = exp/np.sum(exp,axis=0,keepdims=True)\n",
        "      return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J53N9_wWMUf"
      },
      "source": [
        "We have to note that the numerical range of floating point numbers in numpy is limited. For `float64` the upper bound is $10^{308}$. For exponential, its not difficult to overshoot that limit, in which case python returns `nan`.\n",
        "\n",
        "To make our softmax function numerically stable, we simply normalize the values in the vector, by multiplying the numerator and denominator with a constant `C` as\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbf{p}_c  &= \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)} \\\\\n",
        "& = \\frac{C\\exp(\\mathbf{z}_c)}{C\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}\\\\\n",
        "& = \\frac{\\exp(\\mathbf{z}_c + \\log C)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k + \\log C)}.\n",
        "\\end{align*}\n",
        "\n",
        "We can choose an arbitrary value for `log(C)` term, but generally `log(C) = max(z)` is chosen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzmyZdoqXO_v"
      },
      "source": [
        "def stable_softmax(Z): \n",
        "    # Z -- K x N numpy.ndarray, K is the number of classes, N is the number of samples\n",
        "    # TODO (this is optional)\n",
        "    # your code goes here  \n",
        "    probs = (np.exp(Z - np.max(Z)) / np.exp(Z - np.max(Z)).sum())\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSGHPdySbpbj"
      },
      "source": [
        "### Derivative of the softmax function\n",
        "\n",
        "We can show that the derivative of the __softmax__ function with respect to any input can be written as \n",
        "\n",
        "$$ \\frac{\\partial \\mathbf{p}_i}{\\partial \\mathbf{z}_j} = \\begin{cases} \\mathbf{p}_i(1-\\mathbf{p}_j) & i = j \\\\ \\mathbf{p}_i (-\\mathbf{p}_j) & i \\ne j. \\end{cases}$$\n",
        "\n",
        "[More info here](https://deepnotes.io/softmax-crossentropy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0qcy_JAArEo"
      },
      "source": [
        "### Multiclass cross entropy loss function [5 pts]\n",
        "\n",
        "We will minimize the cross entropy loss. You will use the true labels and predicted labels of a batch of N samples. \n",
        "\n",
        "The multi-class cross entropy loss for $i^{th}$ sample can be written as \n",
        "$$Loss_i = -\\sum_c \\mathbf{1}(y_i = c) \\log \\mathbf{p}_c $$\n",
        "where $y_i$ is the true label and \n",
        "\n",
        "$$\\mathbf{1}(y_i = c) = \\begin{cases} 1 & y_i =c \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n",
        "is an indicator function. \n",
        "\n",
        "We can find the average loss for a batch of N samples as $Loss=\\frac{1}{N}\\sum_{i=1}^{N} Loss_i$. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebVzE2SAoTB"
      },
      "source": [
        "def MultiClassCrossEntropyLoss(Y_true, probs):\n",
        "  \n",
        "  # TODO \n",
        "  # Write your code here\n",
        "\n",
        "  # probs -- K x N array\n",
        "  # Y_true -- 1 x N array \n",
        "  # loss --  sum Loss_i over N samples \n",
        "  m= Y_true.shape[1]\n",
        "  loss = -(1/m) * np.sum(Y_true * np.log(probs))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRPrVLpjFIo"
      },
      "source": [
        "### Derivative of the cross entropy loss \n",
        "\n",
        "Let us assume that $\\mathbf{p} = \\text{softmax}(\\mathbf{z})$. \n",
        "\n",
        "Note that the derivative of the loss w.r.t. $\\mathbf{p}_j$ can be written as \n",
        "$$\\frac{\\partial Loss_i }{\\partial \\mathbf{p}_j} = \\begin{cases} -1/\\mathbf{p}_j & j = y_i \\\\ 0 & j \\ne y_i \\end{cases}. $$\n",
        "\n",
        "Note that we can use _total derivative_ to compute the derivative of the loss for $i$th sample w.r.t. $j$th entry in $\\mathbf{z}$ as\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial Loss_i}{\\partial \\mathbf{z}_j} = \\sum_c \\frac{\\partial Loss_i}{\\partial \\mathbf{p}_c}\\frac{\\partial \\mathbf{p}_c}{\\partial \\mathbf{z}_j}.\n",
        "\\end{align*}\n",
        "\n",
        "From our discussion above, we know that the $\\frac{\\partial Loss_i}{\\partial \\mathbf{p}_c} = 0$ if $c \\ne y_i$. \n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial Loss_i}{\\partial \\mathbf{z}_j} &= -\\frac{1}{\\mathbf{p}_c} \\frac{\\partial \\mathbf{p}_c}{\\partial \\mathbf{z}_j} \\\\\n",
        "& = \\begin{cases} \\mathbf{p}_j - 1 & j = y_i \\\\ \\mathbf{p}_j & j \\ne y_i. \\end{cases}\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, $$\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} Loss_i = \\mathbf{p} - \\mathbf{1}_{y_i}.$$\n",
        "\n",
        "where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "462ORdiFLdz2"
      },
      "source": [
        "### Training data\n",
        "\n",
        "Let us pick training data for multi-class classification. \n",
        "\n",
        "Pick same number of images from each class for training and create arrays for input and output. \n",
        "\n",
        "```\n",
        "# train_x -- N x 784 array of training input\n",
        "# train_y -- N x 1 array of labels \n",
        "```  \n",
        "\n",
        "If you use 1000 images from each class N = 10000. You can increase the number of training samples if you like. You may also use unequal number of images in each class. \n",
        "\n",
        "We also need to transpose the dimension of the data so that their size becomes $784\\times N$. It will be helpful to feed it to our model based on our notations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ6Z-yPgL3xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c488f253-0550-4e05-ea16-5583b67948a8"
      },
      "source": [
        "# Pick training samples \n",
        "num_samples = 1000\n",
        "\n",
        "# Training data\n",
        "x = np.zeros((0,784))\n",
        "y = np.zeros((0))\n",
        "for label in range(10):\n",
        "  x1 = x_train[y_train == label]  \n",
        "  x1 = x1[:num_samples]\n",
        "  y1 = y_train[y_train == label]\n",
        "  y1 = y1[:num_samples]\n",
        "  \n",
        "  x = np.concatenate((x,x1),axis=0)\n",
        "  y = np.concatenate((y,y1),axis=0)\n",
        "\n",
        "train_x = x\n",
        "train_y = y\n",
        "print(\"Training data shape:\", train_x.shape)\n",
        "\n",
        "\n",
        "# Test data\n",
        "test_x = x_test\n",
        "test_y = y_test \n",
        "print(\"Test data shape:\", test_x.shape)\n",
        "\n",
        "# reshape data \n",
        "train_x = train_x.T\n",
        "test_x = test_x.T\n",
        "print(\"Training data shape:\", train_x.shape) \n",
        "print(\"Test data shape:\", test_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (10000, 784)\n",
            "Test data shape: (10000, 784)\n",
            "Training data shape: (784, 10000)\n",
            "Test data shape: (784, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can convert the array of labels into an array of one hot vectors as follows\n",
        "N = len(train_y)\n",
        "onehot_y = np.zeros((10,N))\n",
        "onehot_y[train_y.astype(int),np.arange(N)] = 1"
      ],
      "metadata": {
        "id": "U-OQ26bu6cSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot vectors conversion for Test data\n",
        "N = len(test_y)\n",
        "onehot_test_y = np.zeros((10,N))\n",
        "onehot_test_y[test_y.astype(int),np.arange(N)] = 1"
      ],
      "metadata": {
        "id": "G4ZjXu37kO1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoXHKji6L3xo"
      },
      "source": [
        "### Network Architecture\n",
        "\n",
        "We will be using a two layer neural network in our experiment. The input layer has 784 nodes, the hidden layer will have 256 nodes and the output layer will have 10 nodes. First layer will have __sigmoid__ activation and second layer will have __softmax__ activation.\n",
        "\n",
        "The equations for feedforward operation will be as follows.\n",
        "\n",
        "$$\\mathbf{z}^{(1)}=W^{(1)} \\mathbf{x}+ \\mathbf{b}^{(1)}\\\\\\mathbf{y}^{(1)}=\\text{sigmoid}(\\mathbf{z}^{(1)})\\\\\\mathbf{z}^{(2)}=W^{(2)}  \\mathbf{y}^{(1)}+ \\mathbf{b}^{(2)} \\\\\\mathbf{p} = \\mathbf{y}^{(2)}=\\text{softmax}(\\mathbf{z}^{(2)})$$\n",
        "\n",
        "where $\\mathbf{x}\\in \\mathbb{R}^{784}$ is the input layer, $\\mathbf{y}^{(1)}\\in \\mathbb{R}^{256}$ is the hidden layer, $\\mathbf{y}^{(2)} \\in \\mathbb{R}$ is the output layer, $W^{(1)}\\in \\mathbb{R}^{256\\times 784}$ is the first layer weights, $W^{(2)}\\in \\mathbb{R}^{10\\times 256}$ is the second layer weights, $\\mathbf{b}^{(1)}\\in \\mathbb{R}^{256}$ is the first layer bias, $\\mathbf{b}^{(2)}\\in \\mathbb{R}^{10}$ is the second layer bias vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fqsp1BvL3xp"
      },
      "source": [
        "### Network initialization [5 pts]\n",
        "\n",
        "We initialize the weights for $W^{(1)}$ and $W^{(2)}$ with random values drawn from normal distribution with zero mean and 0.01 standard deviation. We will initialize bias vectors $\\mathbf{b}^{(1)}$ and $\\mathbf{b^{(2)}}$ with zero values. \n",
        "\n",
        "We can fix the seed for random initialization for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu-3CQlRL3xp"
      },
      "source": [
        "def TwoLayerNetwork(layer_dims=[784,256,10]):\n",
        "    # TODO \n",
        "    # Your code goes here\n",
        "\n",
        "    # Fix the seed\n",
        "    np.random.seed(3)\n",
        "    \n",
        "    #Initialize the weights\n",
        "\n",
        "    input_layer =  layer_dims[0]\n",
        "    hidden_layer1= layer_dims[1]\n",
        "    output_layer = layer_dims[2]\n",
        "\n",
        "    params = {\n",
        "       'W1' : np.random.normal(0, 0.01, (hidden_layer1, input_layer)),\n",
        "       'b1' : np.zeros((hidden_layer1, 1)),\n",
        "       'W2' : np.random.normal(0, 0.01, (output_layer, hidden_layer1)),\n",
        "       'b2' : np.zeros((output_layer, 1))\n",
        "       }\n",
        "       \n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av7l9tOKUuAN"
      },
      "source": [
        "### Forward propagation \n",
        "Next, we will write the code for the forward pass for two layer network. Each layer consists of an affine function (fully-connected layer) followed by an activation function. You wil also return the intermediate results ($\\mathbf{x}, \\mathbf{z}^{(1)}, \\mathbf{y}^{(1)}, \\mathbf{z}^{(2)}$) in addition to final output ($\\mathbf{y}^{(2)}$). You will need the intermediate outputs for the backpropagation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLSoTwFkUuAO"
      },
      "source": [
        "def forward(X, params):\n",
        "    \n",
        "    # TODO \n",
        "    # Write your codes here\n",
        "\n",
        "    # X -- 784 x N array \n",
        "    # params -- \n",
        "      # W1 -- 256 x 784 matrix\n",
        "      # b1 -- 256 x 1 vector\n",
        "      # W2 -- 10 x 256 matrix\n",
        "      # b2 -- 10 x 1 scalar \n",
        "    # probs -- 10 x N output \n",
        "      W1 = params['W1']\n",
        "      b1 = params['b1']\n",
        "      W2 = params['W2']\n",
        "      b2 = params['b2']\n",
        "\n",
        "      Z1 = np.dot(W1,X) + b1\n",
        "      Y1 = sigmoid(Z1)\n",
        "      Z2 = np.dot(W2,Y1) + b2\n",
        "      probs = softmax(Z2)\n",
        "\n",
        "      intermediate = {\n",
        "          'X':X,\n",
        "          'Z1':Z1,\n",
        "          'Y1':Y1,\n",
        "          'Z2':Z2\n",
        "      }\n",
        "      return probs, intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3QjvXHIUuAO"
      },
      "source": [
        "### Backpropagration step [10 pts]\n",
        "\n",
        "Now we will implement the backpropagation step for the two layer neural network using softmax layer and loss function. \n",
        "\n",
        "\n",
        "You will need the gradient of the Loss w.r.t. $W^{(l)},\\mathbf{b}^{(l)}$ for $l = 1,2$ for all the training samples.  \n",
        "\n",
        "We saw that we can write the gradient of Loss with respect to $W^{(l)}, \\mathbf{b}^{(l)}$ for a single sample as\n",
        "\n",
        "$$\\nabla_{W^{(l)}} Loss_i = \\delta^{(l)} \\mathbf{y}^{(l-1)T},$$  \n",
        "$$\\nabla_{\\mathbf{b}^{(l)}} Loss_i = \\delta^{(l)},$$\n",
        "\n",
        "\n",
        "where \n",
        "$$\\delta^{(l)} = \\nabla_{\\mathbf{z}^{(l)}} Loss = \\nabla_{\\mathbf{y}^{(l)}} Loss \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n",
        "\n",
        "We saw above that for an $i$th sample, $\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} Loss_i = \\mathbf{p} - \\mathbf{1}_{y_i},$ where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$ and $\\mathbf{p}$ is the outpu probability vector for the $i$th sample. \n",
        "\n",
        "\n",
        "**Once we have the gradients $\\nabla_{W^{(l)}} Loss_i, \\nabla_{\\mathbf{b}^{(l)}} Loss_i$ for all $i$. We can compute their average to compute the gradient of the total loss function as**\n",
        "\n",
        "$$\\nabla_{W^{(l)}} Loss = \\frac{1}{N} \\sum_i \\nabla_{W^{(l)}} Loss_i, $$\n",
        "$$ \\nabla_{\\mathbf{b}^{(l)}} Loss = \\frac{1}{N} \\sum_i  \\nabla_{\\mathbf{b}^{(l)}} Loss_i.$$\n",
        "\n",
        "**Please refer to the slides and lectures for more details.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0hPvx0SUuAP"
      },
      "source": [
        "def backward(Y_true, probs, intermediate, params):\n",
        "    \n",
        "    # Inputs: \n",
        "      # Y_true -- true labels\n",
        "      # probs -- 10 x N output of the last layer\n",
        "      # intermediate -- X, Z1, Y1, Z2 \n",
        "      # params -- W1, b1, W2, b2 \n",
        "    \n",
        "    # Outputs: \n",
        "      # grads -- [grad_W1, grad_b1, grad_W2, grad_b2]\n",
        "    \n",
        "    # TODO \n",
        "    # Write your codes here\n",
        "    W1 = params['W1']\n",
        "    W2 = params['W2']\n",
        "\n",
        "    Y1 = intermediate['Y1']\n",
        "    X = intermediate['X']\n",
        "    Z1 = intermediate['Z1']\n",
        "\n",
        "    m = 1 #X.shape[1]\n",
        "\n",
        "    dif = (probs - Y_true)\n",
        "    grad_W2 = (1/m) * np.dot(dif,Y1.T)\n",
        "    grad_b2 = (1/m) * np.sum(dif, axis=1, keepdims = True)\n",
        "    derivative_sigmoid =  (sigmoid(Z1) * (1 - sigmoid(Z1))) \n",
        "    dff2 = (1/m) * np.dot(W2.T, dif) * derivative_sigmoid\n",
        "\n",
        "    grad_W1 = (1/m) * np.dot(dff2, X.T)\n",
        "    grad_b1 = (1/m) * np.sum(dff2, axis=1, keepdims=True)\n",
        "\n",
        "    grads = {\"grad_W1\": grad_W1,\n",
        "             \"grad_b1\": grad_b1,\n",
        "             \"grad_W2\": grad_W2,\n",
        "             \"grad_b2\": grad_b2}\n",
        "          \n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DTxxcDTvVQD"
      },
      "source": [
        "### Train the model [5 pts]\n",
        "We will use the forward and backward functions defined above with the same optimizer defined in the previous question to train our multi-class classificaiton model. \n",
        "\n",
        "We will specify the number of nodes in the layers, number of epochs and learning rate and initialize the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmhkhUisUuAQ"
      },
      "source": [
        "layer_dims = [train_x.shape[0],256,10]\n",
        "epochs = 100\n",
        "lr = 0.00001\n",
        "\n",
        "params = TwoLayerNetwork(layer_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DZAfG5QUuAQ"
      },
      "source": [
        "Then we train the network for the number of epochs specified above. In every epoch, we will do the following:\n",
        "1. Calculate the forward pass to get estimated labels.\n",
        "2. Use the estimated labels calculate loss. We will be recording loss for every epoch.\n",
        "3. Use backpropagation to calculate gradients.\n",
        "4. Use gradient descent to update the weights and biases.\n",
        "\n",
        "You should store the loss value after every epoch in an array ```loss_history```  and print the loss value after every few epochs (say 20). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1zSP6g8UuAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aae91ca-4bd3-4493-ae4d-fc3d1a64cdbc"
      },
      "source": [
        "# TODO \n",
        "# Write your codes here\n",
        "loss_history = []\n",
        "for i in range(0, epochs):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"train_x, params\" and Outputs: \"Y2, intermediate\".\n",
        "        Y2, intermediate = forward(train_x, params)\n",
        "        \n",
        "        # Cost function. Inputs: \"train_y, Y2, parameters\". Outputs: \"cost\".\n",
        "        cost = MultiClassCrossEntropyLoss(onehot_y, Y2)\n",
        "        loss_history.append(cost)\n",
        " \n",
        "        # Backpropagation. Inputs: \"train_y, Y2, intermediate, params\". Outputs: \"grads\".\n",
        "        grads = backward(onehot_y, Y2, intermediate, params)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"params, grads,l r\". Outputs: \"params\".\n",
        "        params = GD(params, grads, lr)\n",
        "\n",
        "        # Print the cost for every iterations\n",
        "        print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 2.315651\n",
            "Cost after iteration 1: 2.173162\n",
            "Cost after iteration 2: 2.056383\n",
            "Cost after iteration 3: 1.937366\n",
            "Cost after iteration 4: 1.817199\n",
            "Cost after iteration 5: 1.702126\n",
            "Cost after iteration 6: 1.596448\n",
            "Cost after iteration 7: 1.509516\n",
            "Cost after iteration 8: 1.432855\n",
            "Cost after iteration 9: 1.396905\n",
            "Cost after iteration 10: 1.386556\n",
            "Cost after iteration 11: 1.413789\n",
            "Cost after iteration 12: 1.260019\n",
            "Cost after iteration 13: 1.139105\n",
            "Cost after iteration 14: 1.022421\n",
            "Cost after iteration 15: 0.964916\n",
            "Cost after iteration 16: 0.912099\n",
            "Cost after iteration 17: 0.884929\n",
            "Cost after iteration 18: 0.862095\n",
            "Cost after iteration 19: 0.870044\n",
            "Cost after iteration 20: 0.867799\n",
            "Cost after iteration 21: 0.845846\n",
            "Cost after iteration 22: 0.786888\n",
            "Cost after iteration 23: 0.712484\n",
            "Cost after iteration 24: 0.665526\n",
            "Cost after iteration 25: 0.637399\n",
            "Cost after iteration 26: 0.621795\n",
            "Cost after iteration 27: 0.617332\n",
            "Cost after iteration 28: 0.628003\n",
            "Cost after iteration 29: 0.657855\n",
            "Cost after iteration 30: 0.690699\n",
            "Cost after iteration 31: 0.702924\n",
            "Cost after iteration 32: 0.669135\n",
            "Cost after iteration 33: 0.604685\n",
            "Cost after iteration 34: 0.554497\n",
            "Cost after iteration 35: 0.517394\n",
            "Cost after iteration 36: 0.496646\n",
            "Cost after iteration 37: 0.487444\n",
            "Cost after iteration 38: 0.472912\n",
            "Cost after iteration 39: 0.464290\n",
            "Cost after iteration 40: 0.449599\n",
            "Cost after iteration 41: 0.440567\n",
            "Cost after iteration 42: 0.429594\n",
            "Cost after iteration 43: 0.417999\n",
            "Cost after iteration 44: 0.407071\n",
            "Cost after iteration 45: 0.398505\n",
            "Cost after iteration 46: 0.393920\n",
            "Cost after iteration 47: 0.391904\n",
            "Cost after iteration 48: 0.395234\n",
            "Cost after iteration 49: 0.395056\n",
            "Cost after iteration 50: 0.412017\n",
            "Cost after iteration 51: 0.411342\n",
            "Cost after iteration 52: 0.425004\n",
            "Cost after iteration 53: 0.410686\n",
            "Cost after iteration 54: 0.412326\n",
            "Cost after iteration 55: 0.386273\n",
            "Cost after iteration 56: 0.370131\n",
            "Cost after iteration 57: 0.358094\n",
            "Cost after iteration 58: 0.348750\n",
            "Cost after iteration 59: 0.344809\n",
            "Cost after iteration 60: 0.335725\n",
            "Cost after iteration 61: 0.332004\n",
            "Cost after iteration 62: 0.325838\n",
            "Cost after iteration 63: 0.321649\n",
            "Cost after iteration 64: 0.319759\n",
            "Cost after iteration 65: 0.323888\n",
            "Cost after iteration 66: 0.333387\n",
            "Cost after iteration 67: 0.349009\n",
            "Cost after iteration 68: 0.370635\n",
            "Cost after iteration 69: 0.371205\n",
            "Cost after iteration 70: 0.375003\n",
            "Cost after iteration 71: 0.355023\n",
            "Cost after iteration 72: 0.332351\n",
            "Cost after iteration 73: 0.311275\n",
            "Cost after iteration 74: 0.288652\n",
            "Cost after iteration 75: 0.279772\n",
            "Cost after iteration 76: 0.271483\n",
            "Cost after iteration 77: 0.268140\n",
            "Cost after iteration 78: 0.264177\n",
            "Cost after iteration 79: 0.265197\n",
            "Cost after iteration 80: 0.265204\n",
            "Cost after iteration 81: 0.271864\n",
            "Cost after iteration 82: 0.273744\n",
            "Cost after iteration 83: 0.283709\n",
            "Cost after iteration 84: 0.287927\n",
            "Cost after iteration 85: 0.301020\n",
            "Cost after iteration 86: 0.307465\n",
            "Cost after iteration 87: 0.322694\n",
            "Cost after iteration 88: 0.314729\n",
            "Cost after iteration 89: 0.325287\n",
            "Cost after iteration 90: 0.295406\n",
            "Cost after iteration 91: 0.278118\n",
            "Cost after iteration 92: 0.253144\n",
            "Cost after iteration 93: 0.241142\n",
            "Cost after iteration 94: 0.235198\n",
            "Cost after iteration 95: 0.232408\n",
            "Cost after iteration 96: 0.230647\n",
            "Cost after iteration 97: 0.231047\n",
            "Cost after iteration 98: 0.231889\n",
            "Cost after iteration 99: 0.236112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtKFEjTUuAQ"
      },
      "source": [
        "Now we will plot the recorded loss values vs epochs. We will observe the training loss decreasing with the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw14IunrUuAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "abfd1840-004d-4a3f-ab54-4ea141e85e4b"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAl20lEQVR4nO3de3xU9Z3/8ddnJveEXEgChCQQLgEEBAQURAQvdX9iW63VXlxrbVfXVt2t3Vtrt9vtZeu229t23Xbtdq2tWqttvVJrrfXGRVEMyP0aEEgCkhsJCblnvr8/ZtAUSTJAJieZ834+HucxM2fOnPkcD84733O+53vMOYeIiPhXwOsCRETEWwoCERGfUxCIiPicgkBExOcUBCIiPpfgdQGnKi8vz5WUlHhdhojIsLJu3bpa51z+yd4bdkFQUlJCWVmZ12WIiAwrZra/t/d0aEhExOcUBCIiPqcgEBHxOQWBiIjPKQhERHxOQSAi4nMKAhERn/NNEOx8u4l/f2Y7rR3dXpciIjKk+CYIKo+08NOVe9lY2eB1KSIiQ4pvgmDuuBwA1u0/4nElIiJDi2+CICc9iUn56axXEIiI/BnfBAHAvPE5rDtwBN2eU0TkXb4LgoaWTvbUHPO6FBGRIcNnQTASQIeHRER68FUQTMxLJzstUSeMRUR68FUQBALG3HHh8wQiIhLmqyCA8HmC8upmGlo6vC5FRGRI8GUQAKxXq0BEBPBhEMwuyiYYMJ0nEBGJ8F0QpCYFmTE2U0EgIhLhuyCA8OGhjRWNdHaHvC5FRMRzvg2C1s5uth086nUpIiKe82UQLJyYC8Dq8lqPKxER8Z4vgyAvI5kZYzNZsavG61JERDznyyAAWDIln/X7j9DU1ul1KSIinvJvEJTm0xVyrNlT53UpIiKe8m0QzBufQ3pSkJW7dXhIRPzNt0GQlBDg/Em5rNhVo/sTiIiv+TYIAJZOyaeivpV9dS1elyIi4hlfB8GSKfkArFTvIRHxMV8HwfjcdMbnpikIRMTXfB0EEO49tGZvHR1dGm5CRPxJQTAln5aObsr213tdioiIJ3wfBOdPyiUhYKzareEmRMSffB8EGckJzB2Xw2oFgYj4VMyCwMyKzewlM9tmZlvN7I6TLGNmdreZlZvZJjObG6t6+nJhaR5bDjZS19zuxdeLiHgqli2CLuAfnHPTgYXA7WY2/YRllgGlkekW4J4Y1tOrC6fk4xy8ouEmRMSHYhYEzrlDzrn1kedNwHag8ITFrgIecGGvAdlmVhCrmnpzdmEWWamJrFI3UhHxoUE5R2BmJcA5wOsnvFUIVPR4Xcl7wwIzu8XMysysrKZm4H+sgwHjgsm5rC6v1XATIuI7MQ8CM8sAHgM+75w7rVuCOed+6pyb75ybn5+fP7AFRlxYms+hxjb21DTHZP0iIkNVTIPAzBIJh8BDzrnHT7JIFVDc43VRZN6gWzw5D4CVu9R7SET8JZa9hgz4GbDdOfeDXhZbDnwy0ntoIdDonDsUq5r6UjwyjYl56azSsNQi4jMJMVz3BcANwGYz2xCZ98/AOADn3E+AZ4ArgHKgBfh0DOvp1+LSPH5bVkl7VzfJCUEvSxERGTQxCwLn3GrA+lnGAbfHqoZTdWFpPg+s2c+6/UdYNCnP63JERAaF768s7un4cBO6ylhE/ERB0ENGcgLnjMtmdbmCQET8Q0FwgsWT89lc1ciRYx1elyIiMigUBCdYXJqHc/CqhpsQEZ9QEJxgdlEWI1ISWF2ubqQi4g8KghMkBAOcPzGXVbs13ISI+IOC4CQuLM2j8kgr++tavC5FRCTmFAQnsbg0PJ7RKvUeEhEfUBCcREluGoXZqazWcBMi4gMKgpMwMy4szePV8jq6ukNelyMiElMKgl4sLs2jqb2LjZWNXpciIhJTCoJeXDApDzM0GqmIxD0FQS9y0pOYVZTNSt2+UkTinIKgD0un5LOhooGGFg03ISLxS0HQh6VT8gk5NAidiMQ1BUEf5hRnk5WayIqdOjwkIvFLQdCHYCDcjXTFrhoNNyEicUtB0I+lU/Kpbmpn+6Emr0sREYkJBUE/lk4JDzexQr2HRCROKQj6MSozhbMKMlmxq9rrUkREYkJBEIWlU/Ip23eE5vYur0sRERlwCoIoXDQ1n66Q41V1IxWROKQgiMLccTlkJCfw0k4dHhKR+KMgiEJSQoClU/J5fns1oZC6kYpIfFEQROmy6aOpaWpnY2WD16WIiAwoBUGULpqaTzBgPL/9sNeliIgMKAVBlLLTkjivZCR/2qYgEJH4oiA4Be+bPppdh5vZX3fM61JERAaMguAUXHbWaAC1CkQkrigITsG43DSmjh6hIBCRuKIgOEWXTR9N2f4jHDmmm9WISHxQEJyi900fTXfI6eIyEYkbpxQEZhYws8xYFTMczCrMYnRmMs9sftvrUkREBkS/QWBmvzKzTDNLB7YA28zsn2Jf2tAUCBhXzSnk5Z3V1Da3e12OiMgZi6ZFMN05dxT4EPAHYAJwQyyLGuqumVtEV8jx1IaDXpciInLGogmCRDNLJBwEy51znYCvB9yZOmYEs4qyeGxdpdeliIicsWiC4H+BfUA6sNLMxgNHY1nUcHDN3CK2HTrKtoO+/08hIsNcv0HgnLvbOVfonLvChe0HLh6E2oa0K2ePJTFoPLZerQIRGd6iOVl8R+RksZnZz8xsPXBJFJ+7z8yqzWxLL+9fZGaNZrYhMv3radTvmZz0JC6dNpon36yiszvkdTkiIqctmkNDfxU5WfwXQA7hE8XfjuJzvwAu72eZVc65OZHpG1Gsc0i5dl4Rdcc6WLFTN7YXkeErmiCwyOMVwIPOua095vXKObcSqD+D2oa8pVPzyctI4tdlFV6XIiJy2qIJgnVm9hzhIPijmY0ABupYyPlmttHM/mBmMwZonYMmMRjgI/OLeWH7YaoaWr0uR0TktEQTBDcBdwLnOudagCTg0wPw3euB8c652cB/A0/2tqCZ3WJmZWZWVlMztA7DXL9gHAC/fG2/x5WIiJyeaHoNhYAi4F/M7HvAIufcpjP9YufcUedcc+T5M4SvV8jrZdmfOufmO+fm5+fnn+lXD6iinDQumz6aR9YeoK2z2+tyREROWTS9hr4N3AFsi0yfM7N/P9MvNrMxZmaR5+dFaqk70/V64cbzSzjS0snTmw55XYqIyClLiGKZK4A5kZYBZnY/8Cbwz319yMweBi4C8sysEvgqkAjgnPsJcC1wq5l1Aa3Ax51zw/KK5fMn5VI6KoP7X93HNXMLieSbiMiwEE0QAGTzbg+grGg+4Jy7rp/3fwT8KMrvH9LMjE8uKuErT27hzYoG5o7L8bokEZGoRXOy+FvAm2b2i0hrYB1wV2zLGn4+fE4hI5IT+J+XyhmmDRsR8aloThY/DCwEHgceA84nPPaQ9JCenMDtl0zm+e3VfOePO70uR0QkalEdGnLOHQKWH39tZmuBcbEqarj6zJKJVNS3cM/Lexg1IplPXzDhz97v6Arx1eVb2XawkSduu4BAQOcSRMR70Z4jOJF+wU7CzPjGVTOpa+7gG09vIyEY4CPzikhJDFJ/rIPPPriOtfvCp1r21jYzedQIjysWETn9exbrIHgvggHjhx+fw4IJI/nKk1s45xt/4tZfruNDP36FDZUNfP59pQBsqGj0uFIRkbBeWwRm9jtO/oNvQG7MKooDKYlBfnnTAl7bW8+zWw/x3NbDBMx45JaFzCnK5t5Vb7GxooFr5xV5XaqISJ+Hhr53mu8JkBAMsLg0j8WleXzjypmY8c71BbOKsthY2eBtgSIiEb0GgXNuxWAWEs9OPCk8uzibe1ftpa2zm5TEoEdViYiEne45AjkDs4uy6ex2bD+k21yKiPcUBB6YU5wNwMaKBk/rEBEBBYEnxmSlMDozmY2V6jkkIt7r9zqCXnoPNQJlwP8659piUVi8m12UrRaBiAwJ0bQI9gLNwP9FpqNAEzAl8lpOw+zibPbWHqOxpdPrUkTE56K5sniRc+7cHq9/Z2ZvOOfONbOtsSos3h0/T7CpqoELS4fWzXZExF+iaRFkmNk74wpFnmdEXnbEpCofOLsoPJq3Dg+JiNeiaRH8A7DazPYQvqp4AnCbmaUD98eyuHiWmZLIpPx0DTUhIp7rNwicc8+YWSkwLTJrZ48TxD+MVWF+MLs4m1W7a3HO6a5mIuKZaLuPzgNmALOBj5rZJ2NXkn+cU5xNTVM7B+pbvC5FRHwsmu6jDwKTgA1Ad2S2Ax6IXVn+sGhyHgCry2sZn5vucTUi4lfRnCOYD0wfrjeWH8om5qVTkJXCK+W1XL9gvNfliIhPRXNoaAswJtaF+JGZsXhyHq+U19EdUs6KiDeiCYI8YJuZ/dHMlh+fYl2YXywuzaOxtZMtVeo9JCLeiObQ0NdiXYSfXdDjPMHsyEVmIiKDKZruo7ovQQzlZSRzVkEmq3fXcvvFk70uR0R8qNdDQ2a2OvLYZGZHe0xNZqaB9AfQ4sm5rNt/hNaO7v4XFhEZYL0GgXNuceRxhHMus8c0wjmXOXglxr/Fpfl0dId4/a06r0sRER+K6oIyMwua2VgzG3d8inVhfnJeyUiSggFeKa/1uhQR8aFoLij7W+CrwGEgFJntgFkxrMtXUpOCzBufw6rdCgIRGXzRtAjuAKY652Y4586OTAqBAba4NI8dbzdR3aT7/IjI4IomCCoI35FMYmhJ5J4Eq9UqEJFBFs11BHuBl83s90D78ZnOuR/ErCofmjE2k9z0JFbuquHDc4u8LkdEfCSaIDgQmZIik8RAIGAsLs1j1e5aQiFHIKBhqUVkcERzQdnXB6MQCR8eemrDQbYdOsrMwiyvyxERn+g1CMzsh865z5vZ7wj3EvozzrkrY1qZD104JTzcxIpdNQoCERk0fbUIHow8fm8wChEYNSKF6QWZrNxVo+EmRGTQ9BoEzrl1kUeNNTSIlkzJ595Ve2lu7yIjOZpTOCIiZ6bf7qNmVmpmj5rZNjPbe3wajOL8aMmUPLpCjjV7NNyEiAyOaK4j+DlwD9AFXEz4FpW/jGVRfjZ//EjSkoKs3FXjdSki4hPRBEGqc+4FwJxz+51zXwPeH9uy/CspIcCiSbms3K0gEJHBEU0QtJtZANhtZn9jZlcDGf19yMzuM7NqM9vSy/tmZnebWbmZbTKzuadYe9xaMiWf/XUt7Drc5HUpIuID0Y41lAZ8DpgHfAK4MYrP/QK4vI/3lwGlkekWwoefBFg2s4D0pCDf++NOr0sRER/oMwjMLAh8zDnX7JyrdM592jl3jXPutf5W7JxbCdT3schVwAMu7DUg28wKTqn6OJU/IpnbLp7Mc9sO8+oejT0kIrHV1x3KEpxz3cDiGH13IeEB7Y6rjMw7WS23mFmZmZXV1Pjj2PlNiydQmJ3KN5/eTnfoPdfziYgMmL5aBGsjj2+a2XIzu8HMPnx8GozijnPO/dQ5N985Nz8/P38wv9ozKYlBvnD5VLYdOsrj6yu9LkdE4lg05whSgDrgEuADwAcjj2eqCiju8booMk8irpw9ljnF2Xz3jzs51t7ldTkiEqf6CoJRZvb3wBZgc+Rxa+TxpD2BTtFy4JOR3kMLgUbn3KEBWG/cMDO+8oHpVDe1c/eLu70uR0TiVF9jGAQJdxM92XjI/R60NrOHgYuAPDOrJHy7y0QA59xPgGeAK4ByoAX49KkU7hfzxufwkXlF/GzVW3z4nCKmjhnhdUkiEmfMuZP/ppvZeufckOvbP3/+fFdWVuZ1GYOq/lgHl3z/ZUpHZfDrW87XvQpE5JSZ2Trn3PyTvdfXoSH92gwRI9OT+NKyabyx7wiP6sSxiAywvoLg0kGrQvr1kXnFzBufw7ee2U5tc3v/HxARiVKvQeCc6+tiMBlkgYBx19UzOdbezbX3vMqemmavSxKROBFN91EZIqaNyeRXf72AprYurv7xK6zerauOReTMKQiGmfklI3ny9gsoyErlxp+v5Scr9ujKYxE5I7oF1jBUPDKNR289ny88uolv/2EHL++s5vsfncPYrBT21h5j3b4jtHeHyE5NJCs1kdlF2WSlJXpdtogMUb12Hx2q/Nh9tDfOOX67rpKvL99KwIyUpCA1Te89kZybnsRdV8/k8pka00/Er/rqPqoWwTBmZnx0fjELJozk23/YQXJCgAUTczlvwkhGpCTQ2NLJocY2/uPZHXz2l+u5as5Yvn7lDLLTkrwuXUSGELUIfKCzO8SPXyrnRy+WM6soi8duXYSZLhMR8ZPTvaBM4kRiMMDn3zeFf7/6bNYfaOD3mzWkk4i8S0HgI9fMK2LamBF859mddHSFvC5HRIYIBYGPBAPGncumcaC+hV++tt/rckRkiFAQ+MzSKfksnpzH3S/uprG10+tyRGQIUBD4jJnxpSum0djayf+8XO51OSIyBCgIfGjG2CyunD2WB9fsV6tARBQEfvXXF06kpaOb35ZVeF2KiHhMQeBTMwuzOK9kJD9/ZR9d3epBJOJnCgIf+6vFJVQ1tPL89sNelyIiHlIQ+Nhl08dQlJPKfa/s87oUEfGQgsDHggHjxvNLWPtWPVuqGr0uR0Q8oiDwuY+eW0xaUpD7Vr/ldSki4hEFgc9lpSZy3XnjeGJDFW/sG/i7kzrn2FTZwLf/sEOtDpEhSqOPCsfau1j2X6twOP5wxxIyks98dPJQyPHAmn08vLaCnYebgHDoPHLLQs4qyDzj9YvIqdHoo9Kn9OQEvv/R2VQeaeWbT2874/WFQo4vP7mZr/1uGylJQe66eia//9xi0pKCfOLe1ymvbh6AqkVkoCgIBIBzS0bymSWTeOSNCl44g+6kzjm+8tQWHl5bwe0XT+LJ2xZx/YLxzBibxUM3L8DMuP7e1zhQ1zKA1YvImVAQyDv+7rJSpo0ZwT/+diPrDxw55c875/ja8q089PoBPrt0Ev/4F1P/7AY4E/MzeOjmBbR2dPOVp7YMZOkicgYUBPKO5IQgP/nEPDJTE7nup6/x7Jbob2DT1R3ii49t4v41+7llyUS+ePnUk94FbeqYEdx28WRW7Kph3f5TDxsRGXgKAvkzJXnpPH7rImaMzeTWh9Zzz8t76OxnCIq2zm5ue2g9vymr5I5LS/nSsml93grzhoXjGZmexA+f3zXQ5YvIaVAQyHvkZiTzq79eyLKZY/iPZ3dw8fde5sHX9tPW2f1ny4VCjvUHjnDjfWt5btthvn7lDP7usin93g85PTmBzyyZyKrdtazbP/BdVkXk1Kj7qPTKOcdLO6v50YvlrD/QQGZKAqWjRzB+ZBopSUFe2lHNocY2khICfPfaWVw1pzDqdbd0dLHkOy9xVkEmD960IIZbISLQd/fRM+8wLnHLzLhk2mgunjqKNXvrWL7hIPvqjrFmbx1HWjpYPDmff/p/U7n0rNFkpSae0rrTkhL4zJJJ3PXMdsr21TO/ZGSMtkJE+qMWgXimtaObC7/zEoU5qTx+6yKCgb4PKYnI6dMFZTIkpSYF+coHzmJjRQO/eHWf1+WI+JaCQDx15eyxXDJtFN/7404q6nWRmYgXFATiKTPjmx+aSTBgfOnxzQy3Q5Ui8UBBIJ4bm53KF5dNY3V5Lb/RPZRFBp2CQIaE688bx8KJI/nKU1t1bYHIIFMQyJAQCBj/c/08CrNTufn+MvbWaIRSkcES0yAws8vNbKeZlZvZnSd5/1NmVmNmGyLTzbGsR4a2kelJ/OLT5xIw41M/f4Pa5navSxLxhZgFgZkFgR8Dy4DpwHVmNv0ki/7aOTcnMt0bq3pkeBifm869N86nuqmNj/3vGg1MJzIIYtkiOA8od87tdc51AI8AV8Xw+yROnDMuh/tuPJfWjm6u/cmrfPWpLTS3d3ldlkjcimUQFAI9u4BURuad6Boz22Rmj5pZ8clWZGa3mFmZmZXV1NTEolYZYhZNzuO5v1/KjeeX8MBr+1lw1/Pc8kAZD72+n7cb27wuTySuxGyICTO7FrjcOXdz5PUNwALn3N/0WCYXaHbOtZvZZ4CPOecu6Wu9GmLCfzZWNPCbsgpe3llDVUMrwYCxbOYYbr5wInOKs70uT2RY8GrQuSqg51/4RZF573DO1fV4eS/wnRjWI8PU7OJsZhdn45xjT00zvy2r5FevH+DpTYeYXZzNxVPzWTw5j9nF2SQG1RFO5FTFskWQAOwCLiUcAG8Af+mc29pjmQLn3KHI86uBLzrnFva1XrUIBKC5vYtfv1HBUxuq2FzViHOQlZrIXy4Yx6cWlTA6M8XrEkWGlL5aBDEdfdTMrgB+CASB+5xzd5nZN4Ay59xyM/sWcCXQBdQDtzrndvS1TgWBnKihpYPX9tbx1IaD/HHr2wQDxgdnjeUvZozhgsm5jEg5tSGyReKRZ0EQCwoC6cv+umPct/otHltfRXN7FwkBY+74HJbNHMMVZxeopSC+pSAQ3+nsDrF+/xFW7KrhxR3V7Hi7CTM4t2QkV80Zy/vPLiA7LWnQ6mlu7+JQQyuTR2X0eytPkVhQEIjvlVc38/tNh/jdpoOUVzeTGDQumjqKD84ey6XTRpGePHD9Jjq7QyQEDDOjvLqJB9bs5/FIC+XswixuWjyB988qIDEYoKs7RLdzJCcEB+z7RU5GQSAS4Zxj68GjPPlmFcs3HqS6qZ2UxACXTBvFxVNHccHkPMZmp57WelftruX7f9rFxooGAIIBozvkSAoG+MCsAqaPzeRXaw+wt+YYaUlBukOO9q4QScEA//KBs7hh4Xi1FiRmFAQiJ9EdcpTtq+f3mw/xzOa33xnbaGJeOjMKsyjJTWN8bjqjRiQzIiWBzNRE2jq7OXy0jcNH22np6MYAM3h2y9u8/lY9hdmpXDO3EDOjKxQiJy2JD51TSF5GMgChkGPFrhpe3llNcmKQ9KQE1h04wspdNXx0fhH/9qGZah1ITCgIRPoRCjl2Hm7ilfJa1uypY3d1M5VHWghF+b9HXkYyf3vJZD5+XvEp/5CHQo7/fH4X//1iOXOKs7nr6pnMGJt1Glsh0jsFgchp6OgKUdXQSv2xdo62dnG0rZPkhACjMlMYnZlCRlICDodzkJGScMYXsz275RBfeHQTR9u6uGz6aO64tJSZhUMrEOqPdXCsvYuukKO9q5vDR9s52NBK/bEOrji7gAl56V6XKL1QEIgME42tnfzilX38bPVejrZ1MW98DtfMLeL9swrISvXueoijbZ188+lt/KasstdlkhIC3HbRJD67dBIpiTq8NdQoCESGmaNtnTz8+gEeXVfJ7upmkhICXDApl4unjeKiKaMoHpk6aCeWV+yq4c7HNnH4aBufWjSBswpGkBA0EoMBRmemUJidihl865kdLN94kAl56Xz7w2ezYGLuoNQn0VEQiAxTzjk2VzXyxJtVvLijmv11LQCkJwUpzEmlKCeNWUVZLJ2Sz6yibIKBgQuHdfvr+dGL5by0s4bJozL47rWzOGdcTp+fWb27li8/uZkD9S3csmQif3/ZFJ38HiIUBCJx4q3aY6zcVcNbtceoPNJKRX0Lu6qbcA6y0xKZMTaTsVmpjM1OZUJeOjPGZjIxPyPqgKhrbuflnTU8uq6SNXvryElL5OYLJ3LT4glRH+451t7FN3+/nYfXHuCsgkz+82OzmTYm80w2WwaAgkAkjtUf62DV7hpW7a6lvLqZQ42tVDe1c/x/7ZTEAJPyMyjOSaN4ZCqjM1PITEkkIyWBkHNUHmml8kgLW6qOsrGyAeegICuFmxZP4C8XjCMt6fQutnt+22HufHwTja2d3HbRZG6/eDJJCUN/dNimtk4eW1dJyMF5E0ZyVkEmLR1dvLijmue2HqYgK4UvXD5tWGxLTwoCEZ/p6Aqxt7aZbQePsvXgUfbWNFMRaUG0d4Xes3x2WiIT89JZOmUUl0wbxYyxmQQG4DBT/bEO/u3pbTzxZhVTRmfw3WtnM3uI3kOi+mgb96/ZxwNr9tPU9u4d8UakJNDeGaKjO0RuehJ1xzq4aGo+91w/j9Sk4XPYS0EgIkD4nMPRti6a2jppbu/COSjKSY35CK0v7jjMl5/YQnVTO5+7pJTbL55Egsf3jqhpauepDVWU7TvC5qpGqhpaMYNlM8fw2aWTyMtIZu1b9bz+Vj3pSUEunzmGueNy+HVZBV9+YvM7t1TNShseo9sqCETEc42tnXz1qS08ueEgc4qz+dcPTmf2AJ/g7ktXd4hDjW2UVzfz6LpKntv2Np3djpLcNGYWZnF2YRaXTR/NxPyMftf1h82HuOORDYzJSuELl0/lipkFA9KCiiUFgYgMGb/beJAvP7GZo21d5KQlcsHkPC4szeP8iXkD2i22szvE2rfq+dO2w6zcXcOBuha6IpeKZ6clcu3cIq5bMI5JUfzwn8zre+v416e2svNwE2cXZvGlK6axaFLegNQeCwoCERlSGlo6WLGrhpW7alm1u4bqpvA4T4XZqcwqymJMVvjq7YKs8HUKhTmp5KQl0dkdor0rRGf3u+c5mtu62HKwkY0Vjew63ERDSydN7Z3UNXfQ0tFNckKACybnMW3MCIpHpjFuZBrzxucMyEVv3SHHk29W8YM/7aKqoZXbL57E3182ddBaOadCQSAiQ9bxe1G/uqeONXvq2HW4ieqj7TS1d/X/4R5SE4NMKxjByLQkRqQkkJ2WxPmTcrmwNO+0ez5Fq62zm68t38ojb1SwaFIud193zjsDDQ4VCgIRGXaa27t4u7GVyiOtVDW00tASHuspOSFAQjDA8b+5kxMDTC/IYvKo6K+XiJXfllXwL09uIS0pyNXnFPHhuYXMGJs5JIYXVxCIiAyS7YeOcvcLu3lhezUd3SEm5aczuyibswoymTpmBKMyk8lNTyYnLTGqnlONrZ1U1LdQUd9C8ci00x6IsK8giG17SUTEZ84qyOSeT8yjoaWD3206xPPbDrO6vJbH36x6z7LpSUEyUhJIjxy66naO7pCjI3IepL0rREtH9zvL37x4QkxGpFUQiIjEQHZaEjcsHM8NC8cD4eE7yqubqTvWQV1zO7XNHTS3d9Hc1kVzRxdG+K52QTOSEgLhKRhgVGYy40amUZSTxvjctJjUqiAQERkEuRnJ5A6xE8jHDa/BMkREZMApCEREfE5BICLicwoCERGfUxCIiPicgkBExOcUBCIiPqcgEBHxuWE31pCZ1QD7T/PjeUDtAJYzXPhxu/24zeDP7fbjNsOpb/d451z+yd4YdkFwJsysrLdBl+KZH7fbj9sM/txuP24zDOx269CQiIjPKQhERHzOb0HwU68L8Igft9uP2wz+3G4/bjMM4Hb76hyBiIi8l99aBCIicgIFgYiIz/kmCMzscjPbaWblZnan1/XEgpkVm9lLZrbNzLaa2R2R+SPN7E9mtjvymON1rbFgZkEze9PMno68nmBmr0f2+a/NLMnrGgeSmWWb2aNmtsPMtpvZ+X7Y12b2d5F/31vM7GEzS4nHfW1m95lZtZlt6THvpPvXwu6ObP8mM5t7Kt/liyAwsyDwY2AZMB24zsyme1tVTHQB/+Ccmw4sBG6PbOedwAvOuVLghcjreHQHsL3H6/8A/tM5Nxk4AtzkSVWx81/As865acBswtse1/vazAqBzwHznXMzgSDwceJzX/8CuPyEeb3t32VAaWS6BbjnVL7IF0EAnAeUO+f2Ouc6gEeAqzyuacA55w4559ZHnjcR/mEoJLyt90cWux/4kCcFxpCZFQHvB+6NvDbgEuDRyCJxtd1mlgUsAX4G4JzrcM414IN9TfgWu6lmlgCkAYeIw33tnFsJ1J8wu7f9exXwgAt7Dcg2s4Jov8svQVAIVPR4XRmZF7fMrAQ4B3gdGO2cOxR5621gtFd1xdAPgS8AocjrXKDBOdcVeR1v+3wCUAP8PHI47F4zSyfO97Vzrgr4HnCAcAA0AuuI733dU2/794x+4/wSBL5iZhnAY8DnnXNHe77nwv2F46rPsJl9AKh2zq3zupZBlADMBe5xzp0DHOOEw0Bxuq9zCP/1OwEYC6Tz3sMnvjCQ+9cvQVAFFPd4XRSZF3fMLJFwCDzknHs8Mvvw8WZi5LHaq/pi5ALgSjPbR/iw3yWEj59nRw4fQPzt80qg0jn3euT1o4SDId739fuAt5xzNc65TuBxwvs/nvd1T73t3zP6jfNLELwBlEZ6FiQRPrm03OOaBlzkuPjPgO3OuR/0eGs5cGPk+Y3AU4NdWyw5577knCtyzpUQ3rcvOueuB14Cro0sFlfb7Zx7G6gws6mRWZcC24jzfU34kNBCM0uL/Hs/vt1xu69P0Nv+XQ58MtJ7aCHQ2OMQUv+cc76YgCuAXcAe4Mte1xOjbVxMuKm4CdgQma4gfLz8BWA38Dww0utaY/jf4CLg6cjzicBaoBz4LZDsdX0DvK1zgLLI/n4SyPHDvga+DuwAtgAPAsnxuK+BhwmfB+kk3AK8qbf9CxjhnpF7gM2Ee1VF/V0aYkJExOf8cmhIRER6oSAQEfE5BYGIiM8pCEREfE5BICLicwoCkQgz6zazDT2mARuwzcxKeo4iKTKUJPS/iIhvtDrn5nhdhMhgU4tApB9mts/MvmNmm81srZlNjswvMbMXI+O/v2Bm4yLzR5vZE2a2MTItiqwqaGb/FxlL/zkzS40s/7nIPSQ2mdkjHm2m+JiCQORdqSccGvpYj/canXNnAz8iPNIpwH8D9zvnZgEPAXdH5t8NrHDOzSY8/s/WyPxS4MfOuRlAA3BNZP6dwDmR9Xw2Npsm0jtdWSwSYWbNzrmMk8zfB1zinNsbGdTvbedcrpnVAgXOuc7I/EPOuTwzqwGKnHPtPdZRAvzJhW8ogpl9EUh0zn3TzJ4FmgkPE/Gkc645xpsq8mfUIhCJjuvl+alo7/G8m3fP0b2f8Dgxc4E3eoyiKTIoFAQi0flYj8c1keevEh7tFOB6YFXk+QvArfDOfZSzelupmQWAYufcS8AXgSzgPa0SkVjSXx4i70o1sw09Xj/rnDvehTTHzDYR/qv+usi8vyV8h7B/Iny3sE9H5t8B/NTMbiL8l/+thEeRPJkg8MtIWBhwtwvfclJk0OgcgUg/IucI5jvnar2uRSQWdGhIRMTn1CIQEfE5tQhERHxOQSAi4nMKAhERn1MQiIj4nIJARMTn/j8dMTmbYPnurgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4fN1udUuAR"
      },
      "source": [
        "### Evaluation on test data [5 pts]\n",
        "\n",
        "Now we will be evaluating the accuracy we get from the trained model. We feed training data and test data to the forward model along with the trained parameters. \n",
        "\n",
        "Note that, we need to convert the (probability) output of the forward pass into labels before evaluating accuracy. We can assign label based on the maximum probability. \n",
        "\n",
        "We assign estimated labels $$\\hat{y}_i = \\arg \\max_c  \\mathbf{p}_c $$ for every probility vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAhOtQigUuAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4b1b77-709f-4670-c56f-0438e82786de"
      },
      "source": [
        "from numpy.ma.core import argmax\n",
        "from numpy.ma.core import argmax\n",
        "# TODO  \n",
        "\n",
        "def accuracy(test_x, onehot_test_y, params):\n",
        "    Y2, intermediate = forward(test_x , params)\n",
        "    output = np.argmax(Y2,0)\n",
        "    output_given = np.argmax(onehot_test_y,0)\n",
        "    output == output_given\n",
        "    acc = np.mean(output_given == output)*100\n",
        "    return acc\n",
        "\n",
        "print(\"Training accuracy:\",accuracy(train_x, onehot_y, params))\n",
        "print(\"Test accuracy:\",accuracy(test_x, onehot_test_y, params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 94.26\n",
            "Test accuracy: 91.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLjwg5wjMcyB"
      },
      "source": [
        "### Visualize some of the correct/miscalassified images [optional]\n",
        "\n",
        "Now we will look at some images from training and test sets that were misclassified. \n",
        "\n",
        "Training set. \n",
        "Pick example from each class that are correcly and incorreclty classified. \n",
        "True/False Positive/Negatives\n",
        "\n",
        "Test set. \n",
        "Pick examples from each class that are correcly and incorreclty classified. \n",
        "True/False Positive/Negatives\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-NtpDz-Mkub"
      },
      "source": [
        "# TODO \n",
        "# Your code goes here ...\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU1ZfPTjv-jm"
      },
      "source": [
        "### Note about implementation\n",
        "\n",
        "This is a note on two problems I have seen in the past and how they can be easily fixed.\n",
        "\n",
        "1. Summation along different axes ?\n",
        "\n",
        "2. Summation of gradients over samples ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**1. Summation to create probability vectors in the Softmax function**\n",
        "\n",
        "Suppose X is a d x N array, in our case, it is 784 x 10000.\n",
        "\n",
        "`Z2 = W2 Y1 + b2 will be 10 x 10000 array`\n",
        "\n",
        "\n",
        "\n",
        "`softmax(Z2)` will be a `10 x 10000` array in which we want to apply a softmax function on every column of `Z2` by first computing exponential and then normalizing the column to sum to 1, which is needed for it to be a probability vector.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We can do that as\n",
        "```\n",
        "probs = np.exp(Z2) \n",
        "\n",
        "# now you want to sum up each column and divide the column by the sum so that each column is a valid probability vector\n",
        "\n",
        "probs /= np.sum(probs,axis=0,keepdims=True) # this makes sum of each column to 1\n",
        "```\n",
        "\n",
        "The **WRONG** thing to do is\n",
        "```\n",
        "probs /= np.sum(probs) \n",
        "# This is WRONG. np.sum() computes sum of the entire array. \n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**2. Computing gradient for the entire loss function**\n",
        "\n",
        "(this involves summation of N rank-one matrices in our notation.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Suppose you have computed delta1, delta2 properly\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's assume you computed\n",
        "```\n",
        "# delta2 is a 10 x 10000 array\n",
        "# Y1 is a 256 x 10000 array\n",
        "# N is 10000\n",
        "# grad_W2 should be a 10 x 256 array\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "We can expand the formula for the gradient of the overall loss.\n",
        "\n",
        "$$\\nabla_{W^{(2)}} Loss = \\frac{1}{N}\\sum_i \\nabla_{W^{(2)}} Loss_i, $$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\nabla_{W^{(2)}} Loss_i = \\delta^{(2)} y^{(1)T}$$ is the gradient of the loss for $i$th training sample, where $\\delta^{(2)}$ is a column of length 10 and $y^{(1)T}$ is a row of length 256, corresponding to $i$th training sample. Matrix product of column and row gives a a rank-1 matrix of size 10 x 256.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To compute the gradient of loss over all the training samples, we need to average the rank-1 matrices for all N training samples.\n",
        "\n",
        "\n",
        "We can write the code for that as\n",
        "\n",
        "```\n",
        "# Sum gradient of loss for each sample\n",
        "for i in range(N):\n",
        "\tgrad_W2 += (1/N)*delta2[:,i,None].dot(Y1[:,i,None].T)\n",
        "\n",
        "# OR we can compute grad_W2 without for loop as \n",
        "grad_W2 = 1/N*np.dot(delta2,Y1.T)\n",
        "```\n",
        "\n",
        "To see why this is true, you can convince yourself that matrix product of an `M x N` matrix with an `N x K` matrix can be written as a summation of N `M x K` rank-one matricess.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Suppose\n",
        "\n",
        "$A = [\\mathbf{a}_1 ~ \\cdots ~ \\mathbf{a}_N] \\text{ and } B = \\begin{bmatrix} \\mathbf{b}_1^T \\\\ \\vdots \\\\ \\mathbf{b}_N^T \\end{bmatrix},$\n",
        "\n",
        "where$\\mathbf{a}_i, \\mathbf{b}_i$ are columns of length $M, K$, respectively.\n",
        "\n",
        "\n",
        "\n",
        "We can write $AB$ as\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$AB = \\sum_{i = 1}^N \\mathbf{a}_i \\mathbf{b}_i^T.$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Submission instructions\n",
        "1. Download this Colab to ipynb, and convert it to PDF. Follow similar steps as [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) but convert to PDF.\n",
        " - Download your .ipynb file. You can do it using only Google Colab. `File` -> `Download` -> `Download .ipynb`\n",
        " - Reupload it so Colab can see it. Click on the `Files` icon on the far left to expand the side bar. You can directly drag the downloaded .ipynb file to the area. Or click `Upload to session storage` icon and then select & upload your .ipynb file.\n",
        " - Conversion using %%shell. \n",
        " ```\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\n",
        "!jupyter nbconvert --log-level CRITICAL --to pdf name_of_hw.ipynb\n",
        "  ```\n",
        " - Your PDF file is ready. Click 3 dots and `Download`.\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "2. Upload the PDF to Gradescope, select the correct pdf pages for each question. **Important!**\n",
        "\n",
        "3. Upload the ipynb file to Gradescope\n",
        "\n",
        "\n",
        "Notice:\n",
        "In case of errors in conversion, please check your LaTeX and debug. In Markdown, when you write in LaTeX math mode, do not leave any leading and trailing whitespaces inside the dollar signs ($). For example, write `(dollarSign)\\mathbf(dollarSign)(dollarSign)` instead of `(dollarSign)(space)\\mathbf{w}(dollarSign)`. Otherwise, nbconvert will throw an error and the generated pdf will be incomplete. [This is a bug of nbconvert.](https://tex.stackexchange.com/questions/367176/jupyter-notebook-latex-conversion-fails-escaped-and-other-symbols)\n"
      ],
      "metadata": {
        "id": "WecX82dMRrVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended"
      ],
      "metadata": {
        "id": "ub1l1I4ZRqSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bdb758-990b-4473-8a47-ae5b65c7aeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 14.2 kB/114\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 25.8 kB/114\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 43.1 kB/114\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to ppa.launc\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers]\r                                              \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "\r0% [Waiting for headers] [5 InRelease 14.2 kB/18.1 kB 78%]\r0% [Waiting for headers] [5 InRelease 14.2 kB/18.1 kB 78%]\r                                                          \rGet:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,017 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,544 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [920 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,021 kB]\n",
            "Hit:15 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,398 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,313 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,136 kB]\n",
            "Fetched 12.7 MB in 2s (5,426 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package texlive-generic-recommended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --log-level CRITICAL --to pdf fall2022_hw3.ipynb # make sure the ipynb name is correct"
      ],
      "metadata": {
        "id": "M2XuF-qZ_-fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8df3041-d656-476b-91c1-3e3ac273bc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    overwrite base name use for output files.\n",
            "                can only be used when converting one notebook at a time.\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMCJzii39h4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}